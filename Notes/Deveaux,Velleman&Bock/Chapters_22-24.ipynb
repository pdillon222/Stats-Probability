{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:40px;\">Stats: Data & Models (De Veaux, Velleman & Bock)[Chapters 22-24]</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter22:Comparing Groups</h1>\n",
    "\n",
    "-Sometimes, instead of a proportion, we have a quantitative variable measured on two groups and we want to compare their means.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Standard Deviation of a Difference</h1>\n",
    "\n",
    "-The difference we observe between two groups is specific to the data at hand.  But what's the true difference for the general population?  We know that our estimate is likely not exactly right.  We need a standardized way to judge how large the difference is.  This means, we'll need to estimate its standard deviation.\n",
    "\n",
    "-How do we find the estimate?  We know how to estimate the standard deviation of the proportion or mean in each group, but how about the difference?  The answer comes from realizing that we have to start with the variances and the Pythoagorean Theorem of Statistics from Chapter 15.\n",
    "\n",
    "-<strong>The variance of the sum or difference of two independent random variables is the sum of their variances</strong>\n",
    "\n",
    "-We want a standard deviation, not a variance, but that's just a square root away.  We can write symbolically what has been said:\n",
    "\n",
    "$$Var(X-Y)=Var(X)+Var(Y)$$\n",
    "$$SD(X-Y)=\\sqrt{SD^2(X)+SD^2(Y)}=\\sqrt{Var(X)+Var(Y)}$$\n",
    "\n",
    "-This simple formula applies only when X and Y are independent, just as the Pythagorean theorem only works for Right Triangles.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Standard Deviation of the Difference Between Two Proportions</h1>\n",
    "\n",
    "-Fortunately, proportions observed in independent random samples are independent, so we can put the two proportions in for X and Y and add their variances.  Typically, we will represent the two sample proportions as $\\hat{p}_1$ and $\\hat{p}_2$, and the two sample sizes as $n_1$ and $n_2$.\n",
    "\n",
    "-The standard deviations of the sample proportions are:\n",
    "\n",
    "$$SD(\\hat{p}_1)=\\sqrt{\\frac{p_1q_1}{n_1}}$$ and \n",
    "\n",
    "$$SD(\\hat{p}_2)=\\sqrt{\\frac{p_2q_2}{n_2}}$$\n",
    "\n",
    "-So the variance of the difference in the proportions is \n",
    "\n",
    "$$Var(\\hat{p}_1-\\hat{p}_2)=(\\sqrt{\\frac{p_1q_1}{n_1}})^2+(\\sqrt{\\frac{p_2q_2}{n_2}})^2=\\frac{p_1q_1}{n_1}+\\frac{p_2pq_2}{n_2}$$\n",
    "\n",
    "-The standard deviation is the square root of the variance:\n",
    "\n",
    "$$SD(\\hat{p}_1-\\hat{p}_2)=\\sqrt{\\frac{p_1q_1}{n_1}+\\frac{p_2q_2}{n_2}}$$\n",
    "\n",
    "-We usually don't know the true values of $p_1$ and $p_2$.  When we have the sample proportions in hand from the data, we use them to estimate the variances.  So the standard error is:\n",
    "\n",
    "$$SE(\\hat{p}_1-\\hat{p}_2)=\\sqrt{\\frac{\\hat p_1\\hat q_1}{n_1}+\\frac{\\hat p_2\\hat q_2}{n_2}}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions for Comparing Proportions</h1>\n",
    "\n",
    "-Before we move on with tests of this nature, we must conduct our typical sample tests:\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Independence Assumption</strong></li>\n",
    "    <li><strong>Randomization Condition</strong></li>\n",
    "    <li><strong>The 10% Condition</strong></li>\n",
    "    <li><strong>Independent Groups Assumption:</strong>The two groups we're comparing must also be independent of each other.  Usually, the independence of groups is evident from the way the data were collected.</li>\n",
    "    <li><strong>Sample Size</strong></li>\n",
    "    <li><strong>Success/Failure Condition</strong></li>\n",
    "</ul>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">A Confidence Interval for the Difference Between Two Proportions</h1>\n",
    "\n",
    "-For large enough samples, each of our proportions has an approximately Normal sampling distribution.  The same is true of the difference.\n",
    "\n",
    "-Provided that the sampled values are inependent, the samples are independent, and the smaple sizes are large enough, the sampling distribution of $\\hat{p}_1-\\hat{p}_2$ is modeled by a Normal model with mean $\\mu=p_1-p_2$, and the standard deviation:\n",
    "\n",
    "$$SD(\\hat{p}_1-\\hat{p}_2)=\\sqrt{\\frac{p_1q_1}{n_1}+\\frac{p_2q_2}{n_2}}$$\n",
    "\n",
    "-The sampling distribution model and the standard deviation give us all we need to find a margin of error for the difference of proportions--or at least they would if we knew the true proportions, $p_1$ and $p_2$.  However, we don't know the true values, so we'll work with observed proportions, $\\hat{p}_1$ and $\\hat{p}_2$, and use $SE(\\hat{p}_1-\\hat{p}_2)$ to estimate the standard deviation.  The rest is just like a one-proportion z-interval.\n",
    "\n",
    "-When the conditions are met, we are ready to find the confidence interval for the difference of two proportions, $p_1$-$p_2$.  The confidence interval is:\n",
    "\n",
    "$$(\\hat{p}_1-\\hat{p}_2)\\pm z^* x SE(\\hat{p}_1-\\hat{p}_2)$$\n",
    "\n",
    "-Where we find the standard error of the difference: \n",
    "\n",
    "$$SE(\\hat{p}_1-\\hat{p}_2)=\\sqrt{\\frac{p_1q_1}{n_1}+\\frac{p_2q_2}{n_2}}$$\n",
    "\n",
    "from the observed proportions.  The critical values $z^*$ depends on the particular confidence level C that we specify.\n",
    "\n",
    "-P.591 is a good example of this\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Two Sample z-Test: Testing for The Difference Between Proportions</h1>\n",
    "\n",
    "-Typically, our null hypothesis for a Two Sample z-Test will be $H_0:p_1-p_2=0$\n",
    "\n",
    "-We know that the standard error of the difference in proportions is:\n",
    "\n",
    "$$SE(\\hat{p}_1-\\hat{p}_2)=\\sqrt{\\frac{p_1q_1}{n_1}+\\frac{p_2q_2}{n_2}}$$\n",
    "\n",
    "-We would just plug in the numbers, but we can actually do slightly better.  There are two proportions in the standard error formula--but look at the null hypothesis.  It says that these proportions should be equal.  To do a hypothesis test, we assume that the null hypothesis is true.  So there should be just a single $\\hat{p}$ value in the SE formula.\n",
    "\n",
    "-So the two samples should somehow be combined.  Combining the counts to get an overall proportion is called <strong>pooling</strong>.  Whenever we have data from different sources or groups but we believe that they really came from the same underlying population, we pool them to get better estimates.  In general, we find the pooled poportion as:\n",
    "\n",
    "$$\\hat{p}_{pooled}=\\frac{Success_1+Success_2}{n_1+n_2}$$\n",
    "\n",
    "-and use that pooled value to estimate the standard error:\n",
    "\n",
    "$$SE_{pooled}(\\hat{p}_1-\\hat{p}_2)=\\sqrt{\\frac{\\hat{p}_{pooled}\\hat{q}_{pooled}}{n_1}+\\frac{\\hat{p}_{pooled}\\hat{q}_{pooled}}{n_2}}$$\n",
    "\n",
    "-and now we can find the test statistic:\n",
    "\n",
    "$$z=\\frac{(\\hat{p}_1-\\hat{p}_2)-0}{SE_{pooled}(\\hat{p}_1-\\hat{p}_2)}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">A Confidence Interval for the Difference Between Two Means</h1>\n",
    "\n",
    "-Comparing two means is just like comparing two proportions.  The statistic of interest is the difference between the two observed means, $\\bar{y}_1$-$\\bar{y}_2$.\n",
    "\n",
    "-To find the difference between two independent sample means, we add their variances and then take a square root:\n",
    "\n",
    "$$SD(\\bar{y}_1-\\bar{y}_2)=\\sqrt{Var(\\bar{y}_1)+Var(\\bar{y}_2)}$$\n",
    "\n",
    "$$=\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}$$\n",
    "\n",
    "-Of course, we still don't know the true standard deviations for the two groups, $\\sigma_1$ and $\\sigma_2$\n",
    "\n",
    "$$SE(\\bar{y}_1-\\bar{y}_2)=\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}$$\n",
    "\n",
    "-<strong>Because we are working with means, and estimating the standard error of their difference using the data, we shouldn't be surprised that the sampling model is a Student's t.</strong>\n",
    "\n",
    "$$(\\bar{y}_1-\\bar{y}_2)\\pm ME$$ where\n",
    "$$ME=t^* x SE(\\bar{y}_1 - \\bar{y}_2)$$\n",
    "\n",
    "-The formula is almost the same as the confidence interval for the difference of two proportions.  We are now, however using a Student's t-model instead of a Normal model.\n",
    "\n",
    "-We are still missing the degrees of freedom, to allow us to use the t-model.  Unfortunately, this is a strange formula.  The secret is that the model is not actually the t, but something very close to it.\n",
    "\n",
    "-When the conditions are met, the sampling distribution of the standardized sample difference between the means of the independent groups:\n",
    "\n",
    "$$t=\\frac{(\\bar{y}_1-\\bar{y}_2)-(\\mu_1-\\mu_2)}{SE(\\bar{y}_1-\\bar{y}_2)}$$\n",
    "\n",
    "-can be modeled by a Student's t-model with a number of degrees of freedom found with a special formula.  We estimate the standard error with:\n",
    "\n",
    "$$SE(\\bar{y}_1-\\bar{y}_2)=\\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}$$\n",
    "\n",
    "-The actual degree of freedom formula, for this particular version of the t-table:\n",
    "\n",
    "$$df=\\frac{(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2})^2}{\\frac{1}{n_1-1}(\\frac{s^2_1}{n_1})^2+\\frac{1}{n_2-1}(\\frac{s^2_2}{n_2})^2}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "<strong>Independence</strong>: We need independent values in each group and independence between the groups.  We see randomization as evidence of independence.\n",
    "\n",
    "<strong>Normal Population</strong>: Must fulfill the Nearly Normal Condition.  The Normality assumption matters most when sample sizes are small.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Two-Sample t-Test: Testing for the Difference Between Two Means</h1>\n",
    "\n",
    "-Sometimes we want to test whether the population means from two groups could be the same more directly, so we'll need a hypothesis test.  This test is called the 'two-sample t-test' for the difference between two means.  The test statistic looks just like the others we've seen and uses the same standard error as we used for the confidence interval.  It finds the difference between the observed group means and compares this with a hypothesized value for that difference.\n",
    "\n",
    "-Whe we write the null hypothesis, we could write: $H_0: \\mu_1=\\mu_2$, but sometimes we'll want to see if the difference is something other than 0, so we usually write it slightly differently.\n",
    "\n",
    "-We'll focus on the hypothesized difference between two means and we'll call that hypothesized difference $\\Delta_0$ (\"delta naught\").  It's so common for that hypothesized difference to be zero that we often just assume $\\Delta_0=0$.  In that case, we'll write $H_0: \\mu_1 - \\mu_2 =0$, but in general we'll write: $H_0: \\mu_1 - \\mu_2 = \\Delta_0$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Tukey's Quick Test</h1>\n",
    "\n",
    "-To use Tukey's test, one group must have the highest value and the other the lowest.  We just count how many values in the high group are higher than all the values of the lower group.  Add to this the number of values in the low group that are lower than all the values of the higher group.  Now if this total is 7 or more, we can reject the null hypothesis of equal means at $\\alpha=.05$.\n",
    "\n",
    "-This is a remarkably good test.  The only assumption it requires is that the two samples be independent.  It's so simple to do that there's no reason not to do on to check your two-sample t results.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">A Rank Sum Test</h1>\n",
    "\n",
    "-Another distribution-free test of whether two means are equal uses the ranks of the data in the two samples.  Tukey's test ranks all the data and then counts the excesses on either side.  <strong>The Wilcoxon rank sum</strong> test first ranks the combined sample from the two groups together from smallest to largest, assigning each observation from rank from 1 to N, where N is $n_1 + n_2$, the total number of observations in both groups.  The test statistic W, is the sum of the ranks of the first group.  If W is too large or small, it is evidence that the groups do not have equal means.\n",
    "\n",
    "-When the null hypothesis is true, the test statistic, W, has mean $\\mu_2=\\frac{n_1(N+1)}{2}$ and variance $Var(W)=\\frac{n_1n_2(N+1)}{12}$.\n",
    "\n",
    "-For any but very small sample sizes, we can use a z-test with $z=\\frac{W-\\mu_w}{SD(W)}$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">the Pooled t-Test: Everyone into the Pool?</h1>\n",
    "\n",
    "-Just because tow groups are assumed equal doesn't mean their variances are as well.  Generally, knowing the mean doesn't tell you anything about the variance.  There are cases however where the assumptions make some sense--especially in the analysis of designed experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter23:Paired Samples and Blocks</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Paired Data</h1>\n",
    "\n",
    "-Sometimes we are interested in differences in data that is paired in certain ways.  Perhaps the most common way is to compare subjects with themselves before and after a treatment.  When pairs arise from an experiment, the pairing is a type of blocking.  When they arise from an observation, the pairing is a type of matching.\n",
    "\n",
    "-Remember that paired data aren't independent, and there is no test to determine whether the data are paired.  You must determine this from understanding how the data were collected and what they mean.\n",
    "\n",
    "-We can concatenate the differences in pairs, and treat them as if they were the data.  Once we have one column of data to evaluate, we can use a one value t-test.  Mechanically, <strong>a paired t-test is just a one-sample t-test for the means of these pairwise differences</strong>.  The sample size is the number of pairs.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Paired Data Condition:</strong>The data must be paired.  You can't just decide to pair the data, when in fact the samples are independent.  Remember, two-sample t methods aren't valid without independent groups, and paired groups aren't independent.</li>\n",
    "    <li><strong>Independence Assumption:</strong>If the data are paired, the groups are not independent.  The differences must however be independent of one another.  An excellent way to be sure of this, is to generate or sample the data with suitable randomization</li>\n",
    "    <li><strong>Normal Population Assumption:</strong>We need to assume that the population of differences follows a Normal model.  We don't need to check the individual groups.  This condtion is typically easier to accomplish with larger sample sizes.  We may also discover, that even if the non-paired data are skewed or non-symmetrical, the differences may be nearly Normal.</li>\n",
    "</ul>\n",
    "\n",
    "-The steps in testing a hypothesis for paired differences are very much like the steps for a one-sample t-test for a mean:\n",
    "\n",
    "$$H_0: \\mu_d=\\Delta_0$$\n",
    "\n",
    "where the d's are the pairwise differences and $\\Delta_0$ is almost always 0.  We use the statistic:\n",
    "\n",
    "$$t_{n-1}=\\frac{\\bar{d}-\\Delta_0}{SE(\\bar d)}$$\n",
    "\n",
    "where $\\bar{d}$ is the mean of the pairwise differences, n is the number of pairs, and\n",
    "\n",
    "$$SE(\\bar{d})=\\frac{s_d}{\\sqrt{n}}$$\n",
    "\n",
    "-$SE(\\bar{d})$ is the ordinary standard error for the mean, applied to the differences.  When the conditions are met and the null hypothesis is true, we can model the sampling distribution of this statistic with a Student's t-model with n-1 degrees of freedom, and use that model to obtain a P-value.  \n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Confidence Intervals for Matched Pairs</h1>\n",
    "\n",
    "-Paired t-Interval:  When the conditions are met, we are ready to find the confidence interval for the mean of the paired differences.  The confidence interval is:\n",
    "\n",
    "$$\\bar{d} \\pm t_{n-1}^* x SE(\\bar{d})$$\n",
    "\n",
    "-Making confidence intervals for matched pairs follows exactly the steps for a one sample t-interval\n",
    "\n",
    "TI-84 interlude: find a critical t-value for 95% CI with 169 DF - invT(.05/2,169)\n",
    "\n",
    "-A confidence interval is a good way to get a sense for the size of the effect we're trying to understand.  This gives us a plausible range of values for the true mean difference.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Blocking</h1>\n",
    "\n",
    "-Pairing isolates the extra variation and allows us to focus on the individual differences.  Blocking makes it easier to see the variability among treatment groups that is attributable to their responses to the treatment.  A paired design, is actually an example of blocking.\n",
    "\n",
    "-<strong>Matching pairs generally removes so much extra variation that it more than compensates for having only half the degrees of freedom, so it is usually a good choice when you design a study (if applicable).</strong>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">*The Sign Test Again</h1>\n",
    "\n",
    "-Because we have paired data, we've been using a simple t-test for the paired differences.  This suggest that if we want a distribution-free method, it would be natural to compute a sign test on the paired differences and test whether the median of the differences is 0.\n",
    "\n",
    "-This test is very simple, we record a 0 for every paired difference that's negative and a 1 for each positive difference, ignoring pairs for differences that are exactly 0.  We test the associated proportion against H0: p=.5, using a z-test if the number of pairs is at least 20, or compute the exact Binomial probabilities if n < 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter24:Comparing Counts</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Goodness-of-Fit Tests</h1>\n",
    "\n",
    "-How closely do observed data fit a \"null\" model.  A hypothesis test to address this question is called a \"goodness-of-fit\" test.  Goodness-of-fit involves testing a hypothesis.  We have specified a model for the distribution and want to know whether it fits.  There is no single parameter to estimate, so a confidence interval wouldn't make much sense.\n",
    "\n",
    "-If the question were only about on data value and its mean, we could use a one-proportion z-test.  If we have a case where we have 12 hypothesized proportions, we need a test that considers all of them together and gives us an overall idea of whether the observed distribution differs from the hypothesized one.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "-Rather than having an observation for each individual, we typically work with summary counts in categories.\n",
    "\n",
    "-<strong>Counted Data Condition</strong>:  The data must be counts for the categories of a categorical variable.  It sounds rudimentary, but be sure that each cell value is actually a count.\n",
    "\n",
    "-<strong>Independence Assumption</strong>:  The counts in the cells should be independent of each other.  The easiest case is when the individuals who are counted in the cells are sampled independently from some population.  If we want to generalize to a larger population, we should check the Randomization Condition.\n",
    "\n",
    "-We can use the chi-square tests to assess patterns in a table provided the individuals counted are independent.  If we  want to generalize our conclusions to a larger population, the individuals who have been counted should be representative of that population.\n",
    "\n",
    "-<strong>Sample Size Assumption</strong>:  We must have enough data for the methods to work, so we usually check the Expected Cell Frequency Condition.  We should expect to see at least 5 individuals in each cell.  This condition should be that np and nq both be at least 10.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Calculations</h1>\n",
    "\n",
    "-We have observed a count in each category from the data, and have an expected count for each category from the hypothesized proportions.  Are the differences just natural sampling variability, or are they so large that they indicate something important?  It's natural to look at the differences between these observed and expected conts, denoted (Obs - Exp).  We would like to think about the total of the differences, but just adding them won't work because <strong>some differences are positive and some are negative</strong>.  We handle these residuals in the same way we did in regression: we square them.  That gives us positive values and focuses attention on any cells with large differences from what we expeced.  Because the differences between observed and expected counts generally get larger the more data we have, we also need to get an idea of the relative sizes of the differences.  To do that, <strong>we divide each squared difference by the expected count for that cell</strong>.\n",
    "\n",
    "-The test statistic, called the <strong>chi-square statistic</strong>, is found by adding up the squares of the deviations between the observed and expected counts divided by the expected counts:\n",
    "\n",
    "$$x^2=\\sum_{\\text{all cells}}\\frac{(Obs-Exp)^2}{Exp}$$\n",
    "\n",
    "-This family of models, like the student's t-models, differ only in the number of degrees of freedom.  <strong>The number of degrees of freedom for a goodness-of-fit test is n-1</strong>.  Here however, n is not the sample size, but instead is the number of categories.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Chi-Square P-Values</h1>\n",
    "\n",
    "-The chi-square statistic for tables of counts is used only for testing hypotheses, <strong>not for constructing confidence intervals</strong>.  If the observed counts don't match the expected, the statistic will be large.  It can't be \"too small\", that would just mean that our model really fit the data well.  The chi-square test is always one-sided.  If the calculated statistic value is large enough, we'll reject the null hypothesis.\n",
    "\n",
    "-As with the t-tables, we have only selected probabilities, so the best we can do is to trap a P-value between two of the values in a table.  \n",
    "\n",
    "-Even though its mechanics work like a one-sided test, the interpretation of a chi-square test is in some sense many-sided.  With more than two proportions, there are many ways the null hypothesis can be wrong.  By squaring the differences, we made all the deviations positive, whether our observed counts were higher or lower than expected.  There's no direction to the rejection of the null model.  All we know is that it doesn't fit.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Chi-Square Calculation</h1>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Find the expected values.</strong>  These come from the null hypothesis model.  Every model gives a hypothesized proportion for each cell.  The expected value is the product of the total number of observations times this proportion.</li>\n",
    "    <li><strong>Compute the residuals.</strong>  Once you have expected values for each cell, find the residuals, <em>Observed-Expected</em></li>\n",
    "    <li><strong>Square the residuals.</strong></li>\n",
    "    <li><strong>Compute the components.</strong>  Now find the component, $\\frac{(Observed-Expected)^2}{Expected}$, for each cell.</li>\n",
    "    <li><strong>Find the sum of the components.</strong>  That's the chi-square statistic.</li>\n",
    "    <li><strong>Find the degrees of freedom.</strong>  It's equal to the number of cells minus one.</li>\n",
    "    <li><strong>Test the hypothesis.</strong>  Large chi-square values mean lots of deviation from the hypothesized model, so they give small P-values.  Look up the critical value from a talbe of chi-square values, or use technology to find the P-value directly.</li>\n",
    "</ul>\n",
    "\n",
    "-In an $x^2$ calculation, in every cell any deviation from the expected count contributes to the sum.  Large deviations generally contribute more, but if there are alot of cells, even small deviations can add up, making the $x^2$ larger.  So the more cells ther are, the higher the value of $x^2$ has to get before it becomes noteworthy.  For $x^2$ then, the decision about how big is big depends on the number of degrees of freedom.\n",
    "\n",
    "-Unlike the Normal and t families, $x^2$ models are skewed:\n",
    "<ul>\n",
    "    <li>The mode is at $x^2$=df-2</li>\n",
    "    <li>The expected value (mean) of a $x^2$ model is its number of degrees of freedom.  That's a bit to the right of the mode--as we would expect for a skewed distribution.</li>\n",
    "</ul>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Trouble with Goodness-of-Fit Tests: What's the Alternative? </h1>\n",
    "\n",
    "-Goodness of fit tests are likely to be performed by people who have a theory of what the proportions should be in each category and who believe their theory to be true.  Unfortunately, the only null hypothesis available for a goodness of fit test is that the theory is true.  As we know, the hypothesis-testing procedure allows us only to reject the null, or fail to reject it.  We can never confirm that a theory is in fact true.  \n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Chi-Square Test of Homogenity</h1>\n",
    "\n",
    "-We already know how to test whether two proportions are the same.  We could use a two proportion z-test, the z-test for two proportions generalizes to a chi-square test of homogenity.  The goodness-of-fit test cmpared counts with a theoretical model, but we're asking if the distribution is the same among different groups, so we find the expected counts for each category directly from the data.  As a result, we count the degrees of freedom slightly differently as well.\n",
    "\n",
    "-The homogenity test comes with a built in hypothesis: We hypothesize that the distribution does not change from group to group.  The test looks for differences large enough to step beyond what we might expect from random sample-to-sample test variation.  It can reveal a large deviation in a single category or small, but persistent, differences over all the categories.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "-The assumptions and conditions are the same as for the chi-square test for goodness-of-fit.  The Counted Data Condition says that these data must be counts.  You can't do a test of homogenity on proportions, you must work with counts.  Also, you can't do a chi-square test on measurements.\n",
    "\n",
    "-We would ideally like the cases to be selected randomly.  We would like to know whether the Independence Assumption both within and across groups is reasonable.\n",
    "\n",
    "-Finally, we must have enough data for this method to work.  The Expected Cell Frequency Conditiono says that the expected count in each cell must be at least 5.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Calculations</h1>\n",
    "\n",
    "-<strong>The null hypothesis states that the distribution among various proportions is the same, the chi-square test seeks to determine if the groups are in fact different and by how much.</strong>\n",
    "\n",
    "-For degrees of freedom, we don't really need to calculate all of the expected values in the table.  To fill out the table, we need to know the counts in only R-1 rows and C-1 columns.  So the table will have (R-1)(C-1) degrees of freedom.  We will obviously need the degrees of freedom to find a P-value for the chi-square statistic.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Examining the Residuals</h1>\n",
    "\n",
    "-Whenever we reject the null hypothesis, it's a good idea to examine the residuals.  For chi-square tests, we want to compare the residuals for celss that may have very different counts, so we're better off standardizing the residuals.  We know that the mean residual will be zero, but we need to know the residual's standard deviation.\n",
    "\n",
    "-<strong>To standardize a cell's residual, we just divide by the square root of its expected value:</strong>\n",
    "\n",
    "$$c=\\frac{(Obs - Exp)}{\\sqrt{Exp}}$$\n",
    "\n",
    "-Notice that these standardized residuals are just the square roots of the components we calculated for each cell, and their sign indicates whether we observed more cases that we expected, or fewer.\n",
    "\n",
    "-Now that we've subtracted the mean (as the residual, Obs - Exp, has mean 0) and divided by their standard deviations, these are z-scores.  If the null hypothesis were true, we could even appeal to the Central Limit Theorem, think of the Normal Model and use the 86-95-99.7 rule.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Chi-Square Test of Independence</h1>\n",
    "\n",
    "-Remember that Contingency Tables categorize counts on two (or more) variables so that we can see whether the distribution of counts on one variable is contingent on the other.\n",
    "\n",
    "-The natural question to ask of these data is whether the chance of A is independent of B.  Recall that for events A and B to be independent; P(A) must equal P(A|B).  \n",
    "\n",
    "-The difference in the test now, is that we have two categorical variables measured on a single population.  For the homogenity test, we had a single categorical variable measured independently on two or more populations.  So now we are asking, \"are the variables independent?\" rather than \"are the groups homogenous?\"\n",
    "\n",
    "<strong>-When we ask whether two variables measured on the same population are independent, we're performing a chi-square test of independence.</strong>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "-We need counts so that the data are at least five in each cell, we want to check to make sure the data are a representative random sample from, and fewer than 10% of the population.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Examine the Residuals</h1>\n",
    "\n",
    "-Each cell in the contingency table contributes a term to the chi-square sum.  If we reject the null hypothesis, we should examine the residuals.  Remember that the ch-square value is the sum of the squares of all values.\n",
    "\n",
    "-The formula for the chi-square standardized residuals divides each residual by the square root of the expected frequency.  Too small an expected frequency can arbitrarily inflate the residual and lead to an inflated chi-square statistic.  \n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Chi-Square and Causation</h1>\n",
    "\n",
    "-Tests for independence are especially widespread.  Unfortunately, many people interpret a small P-value as proof of causation.  Just as correlation between quantitative variables does not demonstrate causation, a failure of independence between two categorical variables does not show a cause-and-effect relationship between them, nor should we say that one variable depends on the other.\n",
    "\n",
    "-the chi-square test for independence treats two variables symmetrically.  There is no way to differentiate the direction of any possible causation from one variable to the other.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
