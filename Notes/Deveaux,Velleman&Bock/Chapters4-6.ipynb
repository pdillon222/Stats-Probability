{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:40px;\">Stats: Data & Models (De Veaux, Velleman & Bock)[Chapters 4-6]</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter4:Comparing Distributions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Sometimes, outliers can teach us alot about the data.  Sometimes, they can indicate significant errors in data collection\n",
    "\n",
    "-It can be a good idea to report a dataset both with and without outliers to compare the difference\n",
    "\n",
    "*<strong>Timeplot</strong>: A display of values against time.\n",
    "\n",
    "-A smooth trace in a time-plot can help make light of more local variation\n",
    "\n",
    "*<strong>Moving Average</strong>: This is typically how we smooth a time plot between points, values are averaged around a point in an interval called a \"window\".  To find the value for the next point, we move the window by one point in time and take the new average.  The size of the chosen window, will effect the level of 'smoothing'.\n",
    "\n",
    "*<strong>Exponential Smoothing</strong>: A more sophisticated moving average function, in which more weight is given to recent past values than to older values\n",
    "\n",
    "*<strong>Re-Expression or Transformation</strong>: A simple function can be used to make a skewed distribution more symmetric\n",
    "\n",
    "-Variables that are skewed to the right often benefit from a re-expression by square roots, logs or reciprocals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter5:Standard Deviation as a Ruler & the Normal Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can judge how unusual a data point is, by how many standard deviation units from the mean it is\n",
    "\n",
    "-To standardize a value, we subtract the mean and then divide this difference by the standard deviation.  This standardized unit is referred to as the <strong>Z-Score</strong>, its formula is as follows:\n",
    "$$z=\\frac{y - \\bar y}{s}$$\n",
    "\n",
    "-Data values below the mean have negative z-scores.  The further the data is from the mean, the more unusual it is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Shifting & Scaling</h1>\n",
    "\n",
    "-Two steps to finding a z-score: the data are shifted by subtracting the mean.  They, they are rescaled by dividing by the standard deviation.\n",
    "\n",
    "-When we shift the data by adding (or subtracting) a constant to each value, all measures of position (center, percentiles, min, max) will increase (or decrease) by the same constant.  This shift, causes no changes in the measure of spread (IQR, range, stdev).\n",
    "\n",
    "<strong>Rescaling to Adjust the Scale</strong>:<br>\n",
    "-If we wanted to shift in units, we would have to rescale the data.  If we wanted to convert lbs. to kg., we would need to multiply each value by 2.2 (thus rescaling the data from lbs. to kgs.).\n",
    "\n",
    "-Rescaling the data, causes measures of spread to change by the scaling factor exactly. (Figure 5.5, p.117 displays this nicely)\n",
    "\n",
    "-Rescaling data by a constant (addition or substraction) does not effect the spread at all.  It only effects measures of position.\n",
    "\n",
    "-When we subtract the mean of the data from every data value, we shift the mean to zero.  As we have seen, such a shift doesn't change the standard deviation.\n",
    "\n",
    "-Standardizing into z-scores does not change the <strong>shape</strong> of the distribution of a variable<br>\n",
    "-Standardizing into z-scores changes the <strong>center</strong> by making the mean = 0<br>\n",
    "-Standardizing into z-scores changes the <strong>spread</strong> by making the standard deviation 1\n",
    "\n",
    "-How far from 0 does a z-score have to be to be interesting or unusual?  <strong>We know that 50% of the data lie between the quartiles.  For symmetric data, the standard deviation is usually a bit smaller than the IQR, and it's not uncommon for at least half of the data to have z-scores between -1 and 1</strong>\n",
    "\n",
    "-To say more about how big we expect a z-score to be, we need to model the data's distribution.  A model will not match the physical world exactly, but will still be useful.\n",
    "\n",
    "-<strong>The Normal Model</strong> is extremely useful and appropriate for distributions whose shapes are unimodal and roughly symmetric.  For these distributions, they provide a measure of how extreme a z-score is.  <strong>There is a Normal model for every possible combination of mean and standard deviation</strong>.\n",
    "\n",
    "-We write $N(\\mu\\sigma)$ to represent a Normal model with a mean of $\\mu$ and a stdev of $\\sigma$.  These numbers don't come from the data, they come from the model.  Such numbers are considered <strong>parameters</strong> of the model.\n",
    "\n",
    "-If we model data with a Normal model and standardize them using the corresponding $\\mu$ and $\\sigma$ we still call the value a z-score and write:\n",
    "\n",
    "$$z = \\frac{y-\\mu}{\\sigma}$$\n",
    "\n",
    "-The Normal model with mean 0 and standard deviation 1 is called the <strong>Standard Normal Model</strong> or the <strong>Standard Normal Distribution</strong>.  When we use the Normal Model, we assume the distribution of the data set is Normal (<strong>Normality Assumption</strong>).\n",
    "\n",
    "-To use the Normal Model, we use the <strong>Nearly Normal Condition</strong>:  The shape of the data's distribution is unimodal and symmetric\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The 68-95-99.7 Rule</h1>\n",
    "\n",
    "-It turns out that <strong>in a Normal Model, about 68% of the values fall within 1 standard deviation of the mean, about 95% of the values fall within 2 standard deviations of the mean and about 99.7%--almost all--of the values fall within 3 standard deviations of the mean.</strong>\n",
    "\n",
    "*<strong>Inflection Point</strong>: The place where the bell shaped curve of the Normal Distribution changes from curving downward to curving back up.  This is exactly one standard deviation from the mean.\n",
    "\n",
    "-What if we are dealing with a value that is not exactly one z-value away from the mean?  We can look it up in a table of Normal Percentiles.  First, the value must be converted to a z-score.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pandas:\n",
      "\n",
      "NAME\n",
      "    pandas\n",
      "\n",
      "DESCRIPTION\n",
      "    pandas - a powerful data analysis and manipulation library for Python\n",
      "    =====================================================================\n",
      "    \n",
      "    See http://pandas.pydata.org/ for full documentation. Otherwise, see the\n",
      "    docstrings of the various objects in the pandas namespace:\n",
      "    \n",
      "    Series\n",
      "    DataFrame\n",
      "    Panel\n",
      "    Index\n",
      "    DatetimeIndex\n",
      "    HDFStore\n",
      "    bdate_range\n",
      "    date_range\n",
      "    read_csv\n",
      "    read_fwf\n",
      "    read_table\n",
      "    ols\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _period\n",
      "    _sparse\n",
      "    _testing\n",
      "    _version\n",
      "    algos\n",
      "    compat (package)\n",
      "    computation (package)\n",
      "    core (package)\n",
      "    formats (package)\n",
      "    hashtable\n",
      "    index\n",
      "    indexes (package)\n",
      "    info\n",
      "    io (package)\n",
      "    json\n",
      "    lib\n",
      "    msgpack (package)\n",
      "    parser\n",
      "    rpy (package)\n",
      "    sandbox (package)\n",
      "    sparse (package)\n",
      "    stats (package)\n",
      "    tests (package)\n",
      "    tools (package)\n",
      "    tseries (package)\n",
      "    tslib\n",
      "    types (package)\n",
      "    util (package)\n",
      "\n",
      "SUBMODULES\n",
      "    datetools\n",
      "    offsets\n",
      "\n",
      "DATA\n",
      "    IndexSlice = <pandas.core.indexing._IndexSlice object>\n",
      "    NaT = NaT\n",
      "    __docformat__ = 'restructuredtext'\n",
      "    dependency = 'dateutil'\n",
      "    describe_option = <pandas.core.config.CallableDynamicDoc object>\n",
      "    get_option = <pandas.core.config.CallableDynamicDoc object>\n",
      "    hard_dependencies = ('numpy', 'pytz', 'dateutil')\n",
      "    missing_dependencies = []\n",
      "    options = <pandas.core.config.DictWrapper object>\n",
      "    plot_params = {'xaxis.compat': False}\n",
      "    reset_option = <pandas.core.config.CallableDynamicDoc object>\n",
      "    set_option = <pandas.core.config.CallableDynamicDoc object>\n",
      "\n",
      "VERSION\n",
      "    0.18.1\n",
      "\n",
      "FILE\n",
      "    c:\\users\\jamesdillon\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from pandas import Series, DataFrame\n",
    "help(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module statistics:\n",
      "\n",
      "NAME\n",
      "    statistics - Basic statistics module.\n",
      "\n",
      "DESCRIPTION\n",
      "    This module provides functions for calculating statistics of data, including\n",
      "    averages, variance, and standard deviation.\n",
      "    \n",
      "    Calculating averages\n",
      "    --------------------\n",
      "    \n",
      "    ==================  =============================================\n",
      "    Function            Description\n",
      "    ==================  =============================================\n",
      "    mean                Arithmetic mean (average) of data.\n",
      "    median              Median (middle value) of data.\n",
      "    median_low          Low median of data.\n",
      "    median_high         High median of data.\n",
      "    median_grouped      Median, or 50th percentile, of grouped data.\n",
      "    mode                Mode (most common value) of data.\n",
      "    ==================  =============================================\n",
      "    \n",
      "    Calculate the arithmetic mean (\"the average\") of data:\n",
      "    \n",
      "    >>> mean([-1.0, 2.5, 3.25, 5.75])\n",
      "    2.625\n",
      "    \n",
      "    \n",
      "    Calculate the standard median of discrete data:\n",
      "    \n",
      "    >>> median([2, 3, 4, 5])\n",
      "    3.5\n",
      "    \n",
      "    \n",
      "    Calculate the median, or 50th percentile, of data grouped into class intervals\n",
      "    centred on the data values provided. E.g. if your data points are rounded to\n",
      "    the nearest whole number:\n",
      "    \n",
      "    >>> median_grouped([2, 2, 3, 3, 3, 4])  #doctest: +ELLIPSIS\n",
      "    2.8333333333...\n",
      "    \n",
      "    This should be interpreted in this way: you have two data points in the class\n",
      "    interval 1.5-2.5, three data points in the class interval 2.5-3.5, and one in\n",
      "    the class interval 3.5-4.5. The median of these data points is 2.8333...\n",
      "    \n",
      "    \n",
      "    Calculating variability or spread\n",
      "    ---------------------------------\n",
      "    \n",
      "    ==================  =============================================\n",
      "    Function            Description\n",
      "    ==================  =============================================\n",
      "    pvariance           Population variance of data.\n",
      "    variance            Sample variance of data.\n",
      "    pstdev              Population standard deviation of data.\n",
      "    stdev               Sample standard deviation of data.\n",
      "    ==================  =============================================\n",
      "    \n",
      "    Calculate the standard deviation of sample data:\n",
      "    \n",
      "    >>> stdev([2.5, 3.25, 5.5, 11.25, 11.75])  #doctest: +ELLIPSIS\n",
      "    4.38961843444...\n",
      "    \n",
      "    If you have previously calculated the mean, you can pass it as the optional\n",
      "    second argument to the four \"spread\" functions to avoid recalculating it:\n",
      "    \n",
      "    >>> data = [1, 2, 2, 4, 4, 4, 5, 6]\n",
      "    >>> mu = mean(data)\n",
      "    >>> pvariance(data, mu)\n",
      "    2.5\n",
      "    \n",
      "    \n",
      "    Exceptions\n",
      "    ----------\n",
      "    \n",
      "    A single exception is defined: StatisticsError is a subclass of ValueError.\n",
      "\n",
      "CLASSES\n",
      "    builtins.ValueError(builtins.Exception)\n",
      "        StatisticsError\n",
      "    \n",
      "    class StatisticsError(builtins.ValueError)\n",
      "     |  Inappropriate argument value (of correct type).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StatisticsError\n",
      "     |      builtins.ValueError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.ValueError:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    mean(data)\n",
      "        Return the sample arithmetic mean of data.\n",
      "        \n",
      "        >>> mean([1, 2, 3, 4, 4])\n",
      "        2.8\n",
      "        \n",
      "        >>> from fractions import Fraction as F\n",
      "        >>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\n",
      "        Fraction(13, 21)\n",
      "        \n",
      "        >>> from decimal import Decimal as D\n",
      "        >>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\n",
      "        Decimal('0.5625')\n",
      "        \n",
      "        If ``data`` is empty, StatisticsError will be raised.\n",
      "    \n",
      "    median(data)\n",
      "        Return the median (middle value) of numeric data.\n",
      "        \n",
      "        When the number of data points is odd, return the middle data point.\n",
      "        When the number of data points is even, the median is interpolated by\n",
      "        taking the average of the two middle values:\n",
      "        \n",
      "        >>> median([1, 3, 5])\n",
      "        3\n",
      "        >>> median([1, 3, 5, 7])\n",
      "        4.0\n",
      "    \n",
      "    median_grouped(data, interval=1)\n",
      "        Return the 50th percentile (median) of grouped continuous data.\n",
      "        \n",
      "        >>> median_grouped([1, 2, 2, 3, 4, 4, 4, 4, 4, 5])\n",
      "        3.7\n",
      "        >>> median_grouped([52, 52, 53, 54])\n",
      "        52.5\n",
      "        \n",
      "        This calculates the median as the 50th percentile, and should be\n",
      "        used when your data is continuous and grouped. In the above example,\n",
      "        the values 1, 2, 3, etc. actually represent the midpoint of classes\n",
      "        0.5-1.5, 1.5-2.5, 2.5-3.5, etc. The middle value falls somewhere in\n",
      "        class 3.5-4.5, and interpolation is used to estimate it.\n",
      "        \n",
      "        Optional argument ``interval`` represents the class interval, and\n",
      "        defaults to 1. Changing the class interval naturally will change the\n",
      "        interpolated 50th percentile value:\n",
      "        \n",
      "        >>> median_grouped([1, 3, 3, 5, 7], interval=1)\n",
      "        3.25\n",
      "        >>> median_grouped([1, 3, 3, 5, 7], interval=2)\n",
      "        3.5\n",
      "        \n",
      "        This function does not check whether the data points are at least\n",
      "        ``interval`` apart.\n",
      "    \n",
      "    median_high(data)\n",
      "        Return the high median of data.\n",
      "        \n",
      "        When the number of data points is odd, the middle value is returned.\n",
      "        When it is even, the larger of the two middle values is returned.\n",
      "        \n",
      "        >>> median_high([1, 3, 5])\n",
      "        3\n",
      "        >>> median_high([1, 3, 5, 7])\n",
      "        5\n",
      "    \n",
      "    median_low(data)\n",
      "        Return the low median of numeric data.\n",
      "        \n",
      "        When the number of data points is odd, the middle value is returned.\n",
      "        When it is even, the smaller of the two middle values is returned.\n",
      "        \n",
      "        >>> median_low([1, 3, 5])\n",
      "        3\n",
      "        >>> median_low([1, 3, 5, 7])\n",
      "        3\n",
      "    \n",
      "    mode(data)\n",
      "        Return the most common data point from discrete or nominal data.\n",
      "        \n",
      "        ``mode`` assumes discrete data, and returns a single value. This is the\n",
      "        standard treatment of the mode as commonly taught in schools:\n",
      "        \n",
      "        >>> mode([1, 1, 2, 3, 3, 3, 3, 4])\n",
      "        3\n",
      "        \n",
      "        This also works with nominal (non-numeric) data:\n",
      "        \n",
      "        >>> mode([\"red\", \"blue\", \"blue\", \"red\", \"green\", \"red\", \"red\"])\n",
      "        'red'\n",
      "        \n",
      "        If there is not exactly one most common value, ``mode`` will raise\n",
      "        StatisticsError.\n",
      "    \n",
      "    pstdev(data, mu=None)\n",
      "        Return the square root of the population variance.\n",
      "        \n",
      "        See ``pvariance`` for arguments and other details.\n",
      "        \n",
      "        >>> pstdev([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])\n",
      "        0.986893273527251\n",
      "    \n",
      "    pvariance(data, mu=None)\n",
      "        Return the population variance of ``data``.\n",
      "        \n",
      "        data should be an iterable of Real-valued numbers, with at least one\n",
      "        value. The optional argument mu, if given, should be the mean of\n",
      "        the data. If it is missing or None, the mean is automatically calculated.\n",
      "        \n",
      "        Use this function to calculate the variance from the entire population.\n",
      "        To estimate the variance from a sample, the ``variance`` function is\n",
      "        usually a better choice.\n",
      "        \n",
      "        Examples:\n",
      "        \n",
      "        >>> data = [0.0, 0.25, 0.25, 1.25, 1.5, 1.75, 2.75, 3.25]\n",
      "        >>> pvariance(data)\n",
      "        1.25\n",
      "        \n",
      "        If you have already calculated the mean of the data, you can pass it as\n",
      "        the optional second argument to avoid recalculating it:\n",
      "        \n",
      "        >>> mu = mean(data)\n",
      "        >>> pvariance(data, mu)\n",
      "        1.25\n",
      "        \n",
      "        This function does not check that ``mu`` is actually the mean of ``data``.\n",
      "        Giving arbitrary values for ``mu`` may lead to invalid or impossible\n",
      "        results.\n",
      "        \n",
      "        Decimals and Fractions are supported:\n",
      "        \n",
      "        >>> from decimal import Decimal as D\n",
      "        >>> pvariance([D(\"27.5\"), D(\"30.25\"), D(\"30.25\"), D(\"34.5\"), D(\"41.75\")])\n",
      "        Decimal('24.815')\n",
      "        \n",
      "        >>> from fractions import Fraction as F\n",
      "        >>> pvariance([F(1, 4), F(5, 4), F(1, 2)])\n",
      "        Fraction(13, 72)\n",
      "    \n",
      "    stdev(data, xbar=None)\n",
      "        Return the square root of the sample variance.\n",
      "        \n",
      "        See ``variance`` for arguments and other details.\n",
      "        \n",
      "        >>> stdev([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])\n",
      "        1.0810874155219827\n",
      "    \n",
      "    variance(data, xbar=None)\n",
      "        Return the sample variance of data.\n",
      "        \n",
      "        data should be an iterable of Real-valued numbers, with at least two\n",
      "        values. The optional argument xbar, if given, should be the mean of\n",
      "        the data. If it is missing or None, the mean is automatically calculated.\n",
      "        \n",
      "        Use this function when your data is a sample from a population. To\n",
      "        calculate the variance from the entire population, see ``pvariance``.\n",
      "        \n",
      "        Examples:\n",
      "        \n",
      "        >>> data = [2.75, 1.75, 1.25, 0.25, 0.5, 1.25, 3.5]\n",
      "        >>> variance(data)\n",
      "        1.3720238095238095\n",
      "        \n",
      "        If you have already calculated the mean of your data, you can pass it as\n",
      "        the optional second argument ``xbar`` to avoid recalculating it:\n",
      "        \n",
      "        >>> m = mean(data)\n",
      "        >>> variance(data, m)\n",
      "        1.3720238095238095\n",
      "        \n",
      "        This function does not check that ``xbar`` is actually the mean of\n",
      "        ``data``. Giving arbitrary values for ``xbar`` may lead to invalid or\n",
      "        impossible results.\n",
      "        \n",
      "        Decimals and Fractions are supported:\n",
      "        \n",
      "        >>> from decimal import Decimal as D\n",
      "        >>> variance([D(\"27.5\"), D(\"30.25\"), D(\"30.25\"), D(\"34.5\"), D(\"41.75\")])\n",
      "        Decimal('31.01875')\n",
      "        \n",
      "        >>> from fractions import Fraction as F\n",
      "        >>> variance([F(1, 6), F(1, 2), F(5, 3)])\n",
      "        Fraction(67, 108)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['StatisticsError', 'pstdev', 'pvariance', 'stdev', 'varianc...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\jamesdillon\\anaconda3\\lib\\statistics.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Normal Probability Plots</h1>\n",
    "\n",
    "-There is a specialized graphical display that can help to decide whether or not the Normal Model is appropriate: the <strong>Normal Probability Plot</strong>\n",
    "\n",
    "-If the distribution of the data is roughly Normal, the plot is roughly a diagonal straight line.  Deviations from a straight line indicate that the distribution is not normal.\n",
    "\n",
    "-Don't use a Normal Model when the distribution is not unimodal and symmetric.\n",
    "\n",
    "*<strong>Nearly Normal Condition</strong>: Look at a picture of the data to see that it is unimodal and symmetric\n",
    "\n",
    "-Rules: Don't use the mean and standard deviation when outliers are present.  Don't round results in the midst of a calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter6:Scatterplots, Association and Correlation</h1>\n",
    "\n",
    "*<strong>Scatterplot</strong>: One of the most common data displays, these plots can allow us to see patterns, trends, relationships, and occasional extraordinary values\n",
    "\n",
    "-Direction of an association in a scatterplot is important.  A Pattern that runs from upper left to lower right is said to be negative.  A pattern running the other way (lower left to upper right) is said to be positive.\n",
    "\n",
    "-The second thing to look for, is a scatterplot's <strong>form</strong>.  A plot that appears as a cloud or swarm of points stretched out in a generally consistent, straight form is called linear.\n",
    "\n",
    "-The third feature to look for, is a scatterplot's <strong>strength</strong>.  At one extreme, do points become tightly clustered in a single stream?\n",
    "\n",
    "-One example of such a surprise is an outlier standing away from the overall pattern of the scatterplot.\n",
    "\n",
    "-You should also look for clusters or subgroups that stand away from the rest of the plot or that show a trend in a different direction.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Roles for Variables</h1>\n",
    "\n",
    "-The variable of interest can be described as the <strong>response variable</strong> and the other the <strong>explanatory</strong> or <strong>predictor variable</strong>.\n",
    "\n",
    "-The roles that we choose for variables are more about how we think about them than about the variables themselves.\n",
    "\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Correlation</h1>\n",
    "\n",
    "-Units shouldn't affect our measure of strength in correlation, a natural way to remove the units is to standardize each variable and work with z-scores:\n",
    "\n",
    "$$(Z_x,Z_y)=(\\frac{x-\\bar x}{S_x},\\frac{y-\\bar y}{S_y})$$\n",
    " \n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">the Correlation Coefficient</h1>\n",
    "\n",
    "$$r = \\frac{\\sum z_x z_y}{n - 1} $$\n",
    "\n",
    "-We can summarize both the strength and direction of x and y associations, by summing the product of the z-scores.\n",
    "\n",
    "-<strong>Dividing the sum by (n-1) serves two purposes</strong>:<br>\n",
    "<ol>\n",
    "    <li>It adjusts the strength for the number of points</li>\n",
    "    <li>It makes the correlation lie between values of -1 and 1</li>\n",
    "</ol>\n",
    "\n",
    "-Formula Alternatives: \n",
    "\n",
    "$$r = \\frac{\\sum(x-\\bar x)(y - \\bar y)}{\\sqrt{\\sum(x - \\bar x)^2(y - \\bar y)^2}}=\\frac{\\sum(x - \\bar x)(y - \\bar y)}{(n-1)S_xS_y} $$\n",
    "\n",
    "-The z-score form tends to be the best for intuitively grasping what correlation means computationally.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions for Correlation</h1>\n",
    "\n",
    "-Correlation measures the strength of linear association between two quantitative variables.  In order to discuss correlation, the variables must satisfy three conditions:<br>\n",
    "<ol>\n",
    "    <li><strong>Quantitative Variables Condition:</strong>Correlation is ONLY about quantitative variables</li>\n",
    "    <li><strong>'Straight Enough Condition':</strong>The best check for the assumption that the variables are truly linearly related is to look at the scatterplot to see whether or not it looks relatively straight.</li>\n",
    "    <li><strong>No Outliers Condition:</strong>Outliers can distort the correlation dramatically, making a weak association look strong, and a strong association look weak</li>\n",
    "</ol>\n",
    "\n",
    "-The sign of a correlation coefficient gives the direction of the association\n",
    "\n",
    "-Correlation is ALWAYS between -1 and +1, if a coefficient of exactly -1.0 and +1.0 arose it would indicate a perfectly straight line.\n",
    "\n",
    "-Correlation treats x and y symmetrically, the correlation of x with y is the same as the correlation of y with x.\n",
    "\n",
    "-Correlation has no units.  This fact can be especially appropriatee when the data's units are somewhat vague to begin with.\n",
    "\n",
    "-Correlation is not affected by changes in the center or scale of either variable.  Changing the units or baseline of either variable has no effect on the correlation coefficient.  Correlation depends only on the z-scores, and they are unaffected by changes in center or scale.\n",
    "\n",
    "-Correlation measures the strength of the linear association between the two variables.  Variables can be strongly associated but still have a small correlation if the association isn't linear.\n",
    "\n",
    "-Correlation is sensitive to outliers.  A single outlying variable can make a small correlation large or make a large one small.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Measuring Trend: Kendall's Tau</h1>\n",
    "\n",
    "*<strong>Kendall's Tau</strong>: A statistic designed to assess how close the relationship between two variables is to being monotone.  A monotone relationship is one that consistently increases or decreases, but not necessarily in a linear fashion.  Kendall's Tau meausures monotonicity directly.  For each pair of points in a scatterplot, it records only whether the slope of a line between thos two points is positive, negative or zero (if points have the same x-value, the slope between them is ignored).  <strong>Tau</strong> is the difference between the number of positive slopes and the number of negative slopes divided the the total number of slopes between pairs.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Nonparametric Association: Spearman's Rho</h1>\n",
    "\n",
    "-One of the problems we have seen with the correlation coefficient is that it is very sensitive to violations of the Straight Enough Condition.  Both outliers and bends in the data make it impossible to interpret correlation.  <strong>Spearman's Rho</strong> ($p$) can deal with both of these problems.  Rho replaces the original data values with their ranks within each variable.  That is, it replaces the lowest value in x, with the number 1, the next lowest with 2...\n",
    "\n",
    "-Spearman's rho is the correlation of the two rank variables.  Because this is a correlation coefficient, it must be between -1 and 1.0.\n",
    "\n",
    "**Both Spearman's Rho and Kendall's Tau have advantages over the correlation coefficient r.  For one they can be used even when we know only the ranks. Also, they measure the consistency of the trend between variables, even if the trend is not linear.  They are also not affected much by outliers.\n",
    "\n",
    "-These are both examples of what are called nonparametric of distribution-free methods\n",
    "\n",
    "-The Correlation Coefficient attempts to estimate a particular parameter in the Normal Model for two quantitative variables.\n",
    "\n",
    "-Kendall's Tau and Speaman's Rho are less specific.  They measure association, but there is no parameter that they are tied to and no specific model they require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Correlation != Causation</h1>\n",
    "\n",
    "-A hidden variable that stands behind a relationship and determines it by simultaneously affecting the other two variables is referred to as a lurking variable.\n",
    "\n",
    "-Regardless of the existence (or lack thereof) of a lurking variable, it is important to remember that <strong>correlation coefficients do not prove causation</strong>.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Ladder of Powers</h1>\n",
    "\n",
    "-We can raise each data value in a quantitative variable to the same power\n",
    "\n",
    "<ul>\n",
    "    <li>the 1/2 power is the same as taking the square root</li>\n",
    "    <li>the -1 power is the reciprocal $y^{-1}=\\frac {1}{y}$</li>\n",
    "    <li>Putting these together: $y^{\\frac{-1}{2}}=\\frac{1}{\\sqrt{y}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
