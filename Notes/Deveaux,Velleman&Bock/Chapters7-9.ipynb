{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:40px;\">Stats: Data & Models (De Veaux, Velleman & Bock)[Chapters 7-9]</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter7:Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Least Squares: The Line of \"Best Fit\"</h1>\n",
    "\n",
    "*<strong>Linear Model</strong>: An equation that will give us a straight line through the plot of data points.  We want to find a line that will come closer than any other possible line.\n",
    "\n",
    "*<strong>Predicted Value</strong>: Where the y value of a data point 'should' fall on this line according to the data point's x value and the linear model's equation. The estimated value ($\\hat y$), is distinguished from the actual value (y).  The <strong>residual value</strong> tells us how far off the model's prediction is at that point.  \n",
    "\n",
    "$$Residual = Observed Value - Predicted Value$$\n",
    "\n",
    "-A line that fits well will have the smallest amount of residuals.  We can't just add up all the residuals to see how well we have done, as some of these values are negative.  Therefore, the residuals should be squared.  Squaring also emphasizes large residuals.  The <strong>line of best fit</strong> is the line for which the sum of the squared residuals is the smallest; the <strong>least squares</strong> line.\n",
    "\n",
    "-The algebraic equation for a straight line is:\n",
    "$$y = mx + b$$\n",
    "\n",
    "-Statistics notation is slightly different:\n",
    "$$\\hat y= b_0 + b_1x$$\n",
    "\n",
    "-The b's are called the coefficients of the linear model.  Coefficient $b_1$ is the <strong>slope</strong>, which tells us how rapidly $\\hat y$ changes with respect to $x$.  The coefficient $b_0$ is the intercept, which tells where the line intercepts the y-axis.\n",
    "\n",
    "-Slopes are always expressed in y-units per x-units.  They tell you how the y-variable changes (in its units) for a one-unit change in the x-variable.  The y-intercept serves only as a starting point, since in statistics 0 is typically not a plausible value for x, especially one in which we should expect to see a value for y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Finding the Least Squares Line</h1>\n",
    "\n",
    "-The equation to find the slope of our least squares line is remarkably simple:\n",
    "$$b_1=r\\frac{S_y}{S_x}$$\n",
    "\n",
    "-Correlations don't have units, but slopes do.  <strong>How x and y are measured doesn't affect their correlation, but can change their slope</strong>.  Changing the units of x and y affects their standard deviations directly.  This is how the slope gets its units--from the ratio of the two standard deviations.\n",
    "\n",
    "-To find the y-intercept:\n",
    "$$b_0=\\bar y - b_1\\bar x$$\n",
    "\n",
    "-Least Squares lines are commonly called <strong>Regression Lines</strong>\n",
    "\n",
    "-To use a regression model, we should check the same conditions for regression as we did for correlation:\n",
    "<ul>\n",
    "    <li>Quantitative Variables</li>\n",
    "    <li>'Straight Enough' test</li>\n",
    "    <li>No Outliers</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Regression to the Mean</h1>\n",
    "\n",
    "-When the correlation between x and y is r, and x has a z-score of $z_x$, the predicted value of $y$ is just $\\hat z_y = rz_x$\n",
    "\n",
    "-For example, if a datum was 2 SD's above the mean of x, the formula would tell us to guess r x 2 standard deviations from y.\n",
    "\n",
    "-<strong>When the correlation is $r$, and an x-value lies $k$ standard deviations from its mean, our prediction for $y$ is $r * k$ standard deviations from its mean.</strong>\n",
    "\n",
    "-According to the z-score equation, for linear relationships you can never predict that y will be farther from its mean than x was from its mean.  Because $r$ is always between -1 and 1.  Therefore, each predicted $y$ tends to be closer to its mean than its corresponding $x$ was.\n",
    "\n",
    "*<strong>Regression to the Mean</strong>: There is always a tendency of a response variable to be closer to the mean than the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Examining the Residuals</h1>\n",
    "\n",
    "-$Residual = Observed value - Predicted value$<br>$e = y - \\hat y$\n",
    "\n",
    "-It's important to know where a model failed.  A scatterplot of the residuals vs. the x-values should be very boring: it shouldn't have any interesting features in direction or shape.  It should stretch horizontally, with about the same amount of scatter throughout.  It should show no bends and have no outliers.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Residual Standard Deviation</h1>\n",
    "\n",
    "-The standard deviation of the residuals $s_e$ gives us a measure of how much the points spread around the regression line.  For this to make sense, the residuals should all share the same underlying spread.\n",
    "\n",
    "*<strong>Equal Variance Assumption</strong>: Does our plot thicken?  The usual violation to the assumption is that spread increases as $x$ or the predicted values increase.  To be sure, we check to make sure that the spread is about the same throughout. \n",
    "\n",
    "-We estimate the <strong>standard deviation of the residuals</strong> in a way you would expect:\n",
    "$$s_e=\\sqrt{\\frac{\\sum e^2}{n-2}}$$\n",
    "\n",
    "-It's a good idea to make a histogram of the residuals.  If we see a unimodal, symmetric histogram then we can apply the 68-95-99.7 rule.  We know that 95% of the residuals should be no farther from 0 than $2S_e$.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">$R^2$-The Variation Accounted for by the Model</h1>\n",
    "\n",
    "-The variation in the residuals is the key to assessing how well the model fits the data.\n",
    "\n",
    "-The squared correlation coefficient $R^2$ gives the fraction of the data's variation accounted for by the model\n",
    "\n",
    "-1-$R^2$ is the fraction of the original variation left in the residuals.\n",
    "\n",
    "-Because $R^2$ is a fraction of a whole, it is often given as a percentage.  $R^2$ of .58 would mean that 58% of the variability in the y variable is accounted for by variation in the x variable.\n",
    "\n",
    "-How big should $R^2$ be? The standard deviation of the residuals can give us more information about the usefulness of the regression by telling us how much scatter there is around the line.\n",
    "\n",
    "-$R^2$ of 100% is a perfect fit, with no scatter around the line the $S_e$ would be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Predicting in the Other Direction</h1>\n",
    "\n",
    "-We can not interchange x for y, if our initial regression line solved for y.  The equation does not work bi-directionally.\n",
    "\n",
    "-Conditions: Quantitative variables condition, Straight enough condition, Does the plot thicken, outlier condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter8:Regression Wisdom</h1>\n",
    "\n",
    "-No regression analysis is complete without a display of the residuals to check that the linear model is reasonable.  The fundamental assumption in working with a linear model is that the relationship you are looking at is in fact linear.\n",
    "\n",
    "-Plotting the residuals can sometimes allow us to see bends that were not as readily apparent in the original scatterplot.\n",
    "\n",
    "-When we find a significant bend in our data, we might want to re-express the values by one of the prescribed methods. (log10,ln,sqrt)\n",
    "\n",
    "-An examination of the residuals may help us to discover that there are groups that are different from the general data.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Subsets</h1>\n",
    "\n",
    "-It can be wise to group variables based on extra criteria, and draw separate regression lines for the data. (e.g., sugar vs. calories in cereal, binned by shelf placement in a grocery store).\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Extrapolation: Reaching Beyond the Data</h1>\n",
    "\n",
    "-Suppose we plug a new x value into the regression model, that is very far from the data we originally used to model the regression line.  The farther away the x-value is from $\\bar x$ \n",
    "\n",
    "-Forecasting to attempt to make future predictions can be a very slippery slope (oil price example on p.224).\n",
    "\n",
    "-<strong>Never interpret a regression slope coefficient as predicting how y is likely to change it its x value in the data were changed</strong>\n",
    "\n",
    "-Regression models describe the data as they are, not as they might be under other circumstances.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Outliers, Leverage and Influence</h1>\n",
    "\n",
    "-One of the great values of models, is that they help us to see when and how data values are unusual.\n",
    "\n",
    "-Data can stand out in two ways:\n",
    "<ol>\n",
    "    <li>A data value can have a large residual</li>\n",
    "    <li>A data point can also be unusual if its x-value is far from the mean of the x-values.  Such a point is said to have high leverage.</li>\n",
    "</ol>\n",
    "\n",
    "-A point with high leverage has the potential to change the regression line.  \n",
    "\n",
    "-<strong>How can we tell if a high-leverage point is actually changing the model?  Fit the regression line twice, both with and without the point</strong>\n",
    "\n",
    "-The point is considered <strong>influential</strong> if omitting it from the analysis changes the model enough to make a meaningful difference.\n",
    "\n",
    "-<strong>Influence depends on both leverage and residual</strong>.  A case with high leverage whose y-value sits right on the line fit to the rest of the data is not influential.  Removing this type of case won't change the slope, even if it does affect $R^2$.\n",
    "\n",
    "-A case with moderate leverage but a very large residual can be influential.\n",
    "\n",
    "-If  point has enough leverage, it can pull the line right to it, even if its residual is small.\n",
    "\n",
    "-<strong>A point with so much influence that it pulls the regression line close to it can make a residual deceptively small</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Lurking Variables and Causation</h1>\n",
    "\n",
    "-With observational data, as opposed to data from a designed experiment, there is never any way to be sure that some degree of causation is not being caused by a <strong>lurking variable</strong>\n",
    "\n",
    "-Scatterplots of statistics summarized over groups tend to show less variability than we would see if we measured the same variable on individuals.  This is because the summary statistics themselves vary less than the data on the individuals do.\n",
    "\n",
    "-You should be a bit suspicious of conclusions based on regressions of summary data.  They may look better than they really are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter9:Re-expressing Data</h1>\n",
    "\n",
    "-For a variable, like speed, there is no <em>single</em> natural way to measure speed.  Sometimes we use time over distance; other times wer use the reciprocal, distance over time.\n",
    "\n",
    "-Some re-expressions of data may be easier to think about, and some may be much easier to analyze with statistical methods.  Symmetric distributions are easier to summarize and straight scatterplots are easier to model with regression.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Straightening Scatterplots - The Four Goals</h1>\n",
    "\n",
    "<strong>Goal 1:</strong>Make the distribution of a variable more symmetric.  It's easier to summarize the center of a symmetric distribution, and for nearly symmetric distributions, we can use the mean and standard deviation.  If the distribution is unimodal, then the resulting distribution may be closer to the Normal model, allowing us to use the <strong>68-95-99.7 Rule</strong>.\n",
    "\n",
    "<strong>Goal 2:</strong>Make the spread of several groups more alike, even if their centers differ.  When a re-expression simplifies the structure in several groups at once, it seems to be a natural choice for those data.\n",
    "\n",
    "<strong>Goal 3:</strong>Make the form of a scatterplot more nearly linear.  \n",
    "\n",
    "<strong>Goal 4:</strong>Make the scatter in a scatterplot spread out evenly rather than thickening at one end.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Finding a Good Re-Expression</h1>\n",
    "\n",
    "-It is difficult to say where the center of a skewed distribution is, and linear relationships are simpler and easier to model than curved relationships.\n",
    "\n",
    "-P.253(Table 9.1), expresses the Ladder of Powers with comments pertaining to application.\n",
    "\n",
    "<table style=\"border:5px solid black;\">\n",
    "    <th>Power</th>\n",
    "    <th>Name</th>\n",
    "    <th>Comment</th>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>The square of the data values $y^2$.</td>\n",
    "        <td>Try this for unimodal distributions that are left skewed.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>The raw data, no change at all.</td>\n",
    "        <td>Data that can take on both positive and negative values with no bounds<br> are less likely to benefit from re-expression.</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>1/2</td>\n",
    "        <td>The square root of the data values, $\\sqrt{y}$.</td>\n",
    "        <td>Counts often benefit from a square root re-expression.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\"0\"</td>\n",
    "        <td>The logarithm of y.  It doesn't matter whether<br>you take the log base 10, or the natural log.</td>\n",
    "        <td>Measurements that can not be negative, and especially values that grow by percentage increases such as<br> salaries or populations, often benefit from a log re-expression.  When in doubt, start here.  If your data have zeros try adding a small constant to all values before finding the logs.</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>-1/2</td>\n",
    "        <td>The (negative) reciprocal square root, $\\frac{-1}{\\sqrt{y}}$</td>\n",
    "        <td>An uncommon re-expression but sometimes useful.  Changing the sign to take the negative of the reciprocal square root preserves the direction of relationships, making things simpler.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>-1</td>\n",
    "        <td>The (negative) reciprocal, $\\frac{-1}{y}$</td>\n",
    "        <td>Ratios of two quantities often benefit from a reciprocal.  You have about a 50-50 chance that the original data was taken in the wrong direction.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Plan B: Logarithms</h1>\n",
    "\n",
    "-When none of the data values is zero or negative, logarithms can be a helpful ally in the search for a useful model.  Try taking the logs of both the x- and y- variables.  Then, re-express the data using some combination of x or log(x) vs. y or log(y)\n",
    "\n",
    "<table style=\"border:5px solid black;\">\n",
    "    <th>Model Name</th>\n",
    "    <th>x-axis</th>\n",
    "    <th>y-axis</th>\n",
    "    <th>Comment</th>\n",
    "    <tr>\n",
    "        <td>Exponential</td>\n",
    "        <td>x</td>\n",
    "        <td>log(y)</td>\n",
    "        <td>This model is the \"0\" power in the ladder approach, useful for values that grow by percentage increases.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Logarithmic</td>\n",
    "        <td>log(x)</td>\n",
    "        <td>y</td>\n",
    "        <td>A wide range of x-values, or a scatterplot descending rapidly at the left but leveling off toward the right, may benefit from trying this model.</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>Power</td>\n",
    "        <td>log(x)</td>\n",
    "        <td>log(y)</td>\n",
    "        <td>The Goldilocks model: When one of the ladder's powers is too big and the next is too small, this may be just right</td>\n",
    "    </tr>\n",
    "    \n",
    "-When several groups are made more symmetric or a relationship seen in different groups is made more linear with the same slope, the argument for that re-expression is even stronger.\n",
    "\n",
    "-Measurement errors are often larger when measuring larger quantities than when measuring smaller ones.  Measurements of rates are often plagued by infinities for those who never finish.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Why Not Just Use a Curve?</h1>\n",
    "\n",
    "-When a clearly curved pattern shows up in the scatterplot, why not just fit a curve to the data?  Fitting a curved model is considerably more difficult that a linear model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
