{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:40px;\">Stats: Data & Models (De Veaux, Velleman & Bock)[Chapters 13-15]</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prof Notes:\n",
    "\n",
    "Chapter 13: From Randomness to Probability\n",
    "\n",
    "We are going to look at random events, which almost no people understand intuitively.  First, we need a definition and important fact about random occurrences.  A random experiment is any phenomenon for which\n",
    "\n",
    "We know every possible outcome that can occur. \n",
    "We cannot predict with certainty which particular outcome will occur.\n",
    "For example, flipping a coin is a random experiment.  Suppose we flip a coin into the air and it comes to rest on the floor.  If we cannot predict with certainty whether it is a head or a tail, then this is a random experiment.\n",
    "\n",
    "In order to illustrate an important result for random phenomena consider flipping a fair coin repeatedly.  If the coin is fair then one-half of flips will be tails and the other half will be heads.  What does this mean?  If you flip coin 4 times, any of 5 possibilities could and sometimes do occur:\n",
    "\n",
    "1. All 4 flips could be heads\n",
    "2. Three flips could be heads and one a tail\n",
    "3. Two flips could be heads and two tails\n",
    "4. One flip could be a head and three tails, or\n",
    "5. All 4 flips could be tails.\n",
    "6. In only one of these possibilities are half of flips heads and half tails.\n",
    "\n",
    "Fortunately, there is a mathematical theorem, which explains precisely what one-half of flips will be tails and the other half will be heads means.  It is the Law of Large Numbers.  The Law of Large Numbers says that if one considers any outcome of any random experiment and one repeats the random experiment a large number of times, then the relative frequency of that outcome, which is the number of times the outcome occurs divided by the total number of times the random experiment occurs, will be close to single number.  This single value is the probability of the outcome occurring.  Furthermore, the larger the number of times the random experiment occurs generally the closer the relative frequency will be to that single number.  In the case of flipping a fair coin repeatedly and looking at the relative frequency of heads, the single number to which relative frequency gets close is ½ and only ½.  \n",
    "\n",
    "Every random experiment has a sample space.  The sample space is the set of all of the possible outcomes of the random experiment.  For example, let us roll a fair dice.  The sample space, usually denoted by S, is\n",
    "\n",
    "S = {1, 2, 3, 4, 5, 6}.\n",
    "\n",
    "The members of a set are its elements.  For example, 5 is an element of S.  An event, E, is a set of some, all, or none of the outcomes in S.  For example, let E = {2, 4, 6}.  We say an event, E, occurs if the outcome of the random experiment is in E.  If the die roll is 2, 4, or 6, then E occurs otherwise E does not occur.  This event E is that the die roll is an even number.\n",
    "\n",
    "There are two different kinds of sample spaces.  An equally likely sample space is one in which all of the outcomes in S have exactly the same chance of occurring.  If some of the outcomes in a sample space have a greater chance of occurring than other outcomes do, then the sample space is a non-equally likely sample space.\n",
    "\n",
    "If the die is “fair, ” then S is an equally likely sample space.  If any of the sides of the die is more likely to turn up than some other side, then we have a biased die and S is a non-equally likely sample space.\n",
    "\n",
    "We write the probability of an event E as P(E).  The probability of an event E measures the chance E occurs.  For an event E, the cardinality of the event, written |E|, is the number of elements in E.  For example, if E = {1, 2, 3}, then |E|= 3.  If F= {5, 6}, then |F|= 2.\n",
    "\n",
    "Important Result \n",
    "\n",
    "If S is an equally likely sample space and E is an event, then P of E equals the cardinality of E divided by the cardinality of S.\n",
    "\n",
    "For example for a fair die roll with E and F defined as above,\n",
    "\n",
    "P of E equals the cardinality of E divided by the cardinality of S = 3 over 6\n",
    "\n",
    "$$P(E) = \\frac{|E|}{|S|} = \\frac{|\\{2,4,6\\}|}{|\\{1,2,3,4,5,6\\}|}=\\frac{3}{6}$$\n",
    "\n",
    "And\n",
    "\n",
    "P of F equals the cardinality of F divided by the cardinality of S equals 2 over 6\n",
    "\n",
    "$$P(F) = \\frac{|F|}{|S|} = \\frac{|\\{4,5,6\\}|}{|\\{1,2,3,4,5,6\\}|}=\\frac{2}{6}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter13:From Randomness to Probability</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Random Phenomena</h1>\n",
    "\n",
    "*<strong>Random Phenomenon</strong>: A situation in which we know what outcomes can possibly occur, but we don't know which particular outcome will happen.\n",
    "\n",
    "-The accumulation of random events, when comparing their outcomes to the proportion of possible outcomes the fraction of accumulated experience tends to settle down.\n",
    "\n",
    "-Each occasion upon which we observe a random phenomenon is called a <strong>trial</strong>.  At each trial, we note the value of the random phenomenon, and call that the trial's <strong>outcome</strong>.\n",
    "\n",
    "-When we combine outcomes, the resulting combination is an <strong>event</strong>.  We call the collection of all possible outcomes the <strong>sample space</strong>.\n",
    "\n",
    "-We will denote the sample space as S, however some texts denote sample space as $\\Omega$.\n",
    "\n",
    "-If we were to flip two coins, the sample space would be; S={HH,HT,TH,TT}.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Law of Large Numbers</h1>\n",
    "\n",
    "-Do random phenomena always behave well enough for a probability to make sense?\n",
    "\n",
    "*<strong>Law of Large Numbers</strong>:As we repeat a random process over and over, the proportion of times that an event occurs does settle down to one number.  This number is considered the <strong>probability</strong> of the event.\n",
    "\n",
    "-The Law of Large Numbers requires two key assumptions:\n",
    "<ol>\n",
    "    <li>The random phenomenon we're studying must not change; the outcomes must have the same probabilities for each trial.</li>\n",
    "    <li>The events must be independent.  Informally, independence means that the outcome of one trial doesn't affect the outcomes of the others</li>\n",
    "</ol>\n",
    "\n",
    "-The LLN says that as the number of independent trials increases, the long-run relative frequency of repeated events gets closer to a single value.  Because this definition is based on repeatedly observing the even'ts outcome, it is often called the <strong>empirical probability</strong>.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Nonexistent Law of Averages</h1>\n",
    "\n",
    "-The Law of Large Numbers says nothing about short-run behaviour.  Relative frequencies even out only in the long run.  According to the LLN, the 'Long-run' is extremely long.  In short, the law of averages doesn't actually exist.  There is no Law of Averages for short run events.\n",
    "\n",
    "-Sequences of random events don't compensate in the short run, and don't need to do so to get back to the right long-run probability.  If the probability of an outcome doesn't change and the events are independent, the probability of any outcome in another trial is always what it was, no matter what has happened in the other trials.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Modeling Probability</h1>\n",
    "\n",
    "-When probability comes from a mathematical model and not from observation, it is called <strong>theoretical probability</strong>.\n",
    "\n",
    "-It's easy to find probabilities for events that are made up of several equally likely outcomes.  We just count all the outcomes that the event contains.  The probability of the event is the number of outcomes in the event divided by the total number of possible outcomes.\n",
    "\n",
    "$$P(A) = \\frac{\\# \\text{ outcomes in A}}{\\# \\text{ of possible outcomes}}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Personal Probability</h1>\n",
    "\n",
    "-We use the language of probability in everyday speech to express a degree of uncertainty without basing it on long-run relative frequencies or mathematical models.  We call this third kind of probability a <strong>subjective</strong> or <strong>personal probability</strong>.\n",
    "\n",
    "-The most common kind of picture to make is called a Venn diagram.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Formal Probability</h1>\n",
    "\n",
    "-Formal rules of probality:\n",
    "<ol>\n",
    "    <li><strong>Rule 1</strong>:If the probability is 0, the event never occurs.  Likewise if the probability is 1, it always occurs.  Even if an event is very unlikely, it's probability can't be negative.  $\\text{For any event A,}0 \\leq P(A) \\leq 1$  </li>\n",
    "    <li><strong>Rule 2</strong>:If a random phenomenon has only one possible outcome, it's neither very interesting or random.  If we look at all the events in the entire sample space, the probability of that collection of events has to be 1.  So the probability of an entire sample space is always 1.</li>\n",
    "    <li><strong>Rule 3</strong>:The set of outcomes that are not in the event A, is called the complement of A, and is denoted $A^C$.  Which leads to the complement rule $P(A^C) =1-P(A)$</li>\n",
    "    <li><strong>Rule 4</strong>:What is the probability of either A or B?  This leads to the <strong>addition rule</strong>: you can add the probabilities of events that are <strong>disjoint</strong>.  Disjoint events are mutually exclusive.  For two disjoint events A and B, the probability that one or the other occurs is the sum of the probabilities of the two events. $\\text{P(A or B)}=\\text{P(A) + P(B),provided that A and B are disjoint}$.  The Probability Assignment Rule tells us that to be a legitimate assignment of probabilities, the sum of the probabilities of all possible outcomes must be exactly 1, no more no less.</li>\n",
    "    <li><strong>Rule 5</strong>:For independent events, the outcome of one event doesn't influence the outcome of another. The <strong>multiplication rule</strong> comes into play.  For independent events, to find the probability that both events occur, we just multiply the probabilities together.  $\\text{P(A and B) = P(A) x P(B)}$</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array1=[-1,12,12,11,-2,54,678,23,567,18,19,27]\n",
    "arrayish=[-45,-44,50,-42,-48]\n",
    "array2=[4,-1,-6,-7,8,190,304,-456,809]\n",
    "array3=[5,4,6,90,234,567,12]\n",
    "\n",
    "#currently doesn't work on - negative numbers\n",
    "\n",
    "def sort_array(array):\n",
    "    should_restart = True\n",
    "    while should_restart:\n",
    "        should_restart = False\n",
    "        for i in array:\n",
    "            if array.index(i) == len(array)-1:\n",
    "                break\n",
    "            else:\n",
    "                if i > array[array.index(i)+1]:\n",
    "                    array.insert(array[array.index(i)+1],i)\n",
    "                    array.remove(array[array.index(i)]) #concerned that index will not work if array contains non-unique numbers\n",
    "                    should_restart = True\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "                                                \n",
    "    return array\n",
    "#print(sort_array(array3))\n",
    "\n",
    "#try rewriting using counters and array length, instead of referencing indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -2, -3, -2, -2, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "array=[1,-2,-3,-2,4,5]\n",
    "if array[array.index(-2)] > array[array.index(-3)]:\n",
    "    array.insert(array[array.index(-2)],array[array.index(-2)]) and array.insert(array[array.index(-3)],array[array.index(-3)])\n",
    "    print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter14:Probability Rules</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The General Addition Rule</h1>\n",
    "\n",
    "-If two events are not disjoint, they overlap and therefore we can't use the addition rule.\n",
    "\n",
    "-We can however <strong>add the probabilities of two events and then subtract out the probability of their instersection<strong>.\n",
    "\n",
    "*<strong>General Addition Rule</strong>:(For non-disjoint events)-$\\text{ P(A or B) = P(A) + P(B) - P(A and B)}$\n",
    "\n",
    "-p.370, displays usage of the rule well\n",
    "\n",
    "-if P(A)=.71, P(B)=.18 and P(A and B)=.15:<br>\n",
    "    P(A and $B^C$)=.71 - .15 = .56<br>\n",
    "    P(B and $A^C$)=.18 - .15 = .03<br>\n",
    "    P($A^C$ and $B^C$) = 1 - (.56 + .15 + .03) = .26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Conditional Probability and the General Multiplication Rule</h1>\n",
    "\n",
    "-When we want the probability of an event from a conditional distribution, we write <strong>P(B|A)</strong>.  We pronounce this the probability of P given A.  A probability that takes into account a given condition such as this is considered a <strong>conditional probability</strong>.\n",
    "\n",
    "-To find the probability of the event B given event A, we restrict our attention to the outcomes in A.  We then find in what fraction of those outcomes B also occurred.  Formally, we write:\n",
    "\n",
    "$$P(B|A) = \\frac{\\text{P(A and B)}}{P(A)}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The General Multiplication Rule</h1>\n",
    "\n",
    "-The multiplication rule (for independent events):\n",
    "$$\\text{P(A and B) = P(A) x P(B); when A and B are independent}$$\n",
    "\n",
    "-If events are not necessarily independent, we use the General Multiplication Rule:\n",
    "$$\\text{P(A and B) = P(A) x P(B|A)}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Independence</h1>\n",
    "\n",
    "-Independence means that the outcome of one event does not influence the outcome of another.  Events A and B are independent if and only if <strong>P(B|A) = P(B)</strong>.  The probability of B given A, equals the probability of B.\n",
    "\n",
    "-We should use the formal rule whenever we can.  The Multiplication Rule for independent events is simply a special case of the General Multiplication Rule.  The General Multiplication Rule says that:\n",
    "\n",
    "$$\\text{P(A and B) = P(A) x P(B|A)}$$\n",
    "\n",
    "whether the events are independent or not.  When events are <strong>independent</strong>, we can write:\n",
    "\n",
    "$$\\text{P(A and B) = P(A) x P(B)}$$\n",
    "\n",
    "-The idea is that for independent events, the probability of one doesn't change when the other occurs.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Indepedent $\\neq$ Disjoint</h1>\n",
    "\n",
    "-Both disjoint and independent seem to imply separation and distinctness, but in fact disjoint events cannot be indendependent.\n",
    "\n",
    "-Assuming two events: {you get an A in a course} and {you get a B in a course}.  They are disjoint because they have no outcomes in common.  Suppose you learn that you did get an A in the course.  Now what is the probability you got a B?  You can't get both grades, so it must be 0.  Knowing that the first event (getting an A) occurred changed your probability for the second event (down to 0).  So the events aren't independent.\n",
    "\n",
    "-Mutually exclusive events <strong>are not</strong> independent.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Picturing Probability: Tables, Venn Diagrams and Trees</h1>\n",
    "\n",
    "![title](treediagram.jpg)\n",
    "\n",
    "-Neither tables nor Venn diagrams can handle conditional probabilities.  The kind of picture that helps us look at conditional probabilities is called a <strong>tree diagram</strong>.  It is a good idea to draw out a tree diagram almost any time you plan to use the General Multiplication Rule.\n",
    "\n",
    "-Branches of the tree cover all possible outcomes, notice that at each splitting point the joint probabilities must always add up to 1.\n",
    "\n",
    "-All the outcomes at the far right are disjoint because at each branch or the tree we chose between disjoint alternatives.  These are all the possibilities, so the probabilities on the far right must add up to one.\n",
    "\n",
    "-Because the final outcomes are disjoint, we can add up their probabilities to get probabilities for compound events.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Reversing the Conditioning and Bayes' Rule</h1>\n",
    "\n",
    "-If we know a student has had an alcohol-related accident, what's the probability that the student is a binge drinker?  It's an interesting question, and not just one that we can answer from our tree diagram.\n",
    "\n",
    "*To find a conditional probability, we need the probability that both events happen divided by the probability that the given event occurs.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Bayes' Rule</h1>\n",
    "\n",
    "-When we have <strong>P(A|B)</strong> but want the reverse probability <strong>P(B|A)</strong> we need to find <strong>P(A and B) and P(A)</strong>.\n",
    "\n",
    "-We could write the equation algebraically, showing exactly how we found the quantities we needed: <strong>P(A and B) and P(A).  The result is a formula known as Bayes' Rule.  This is the foundation of an approach to statistics known as Bayesian Statistics.  Bayes' Rule is simply a formula for reversing the probability from the conditional probability that you're originally given.\n",
    "\n",
    "-Bays Rule for two events says that:\n",
    "\n",
    "$$P(B|A) = \\frac{\\text{P(A|B)P(B)}}{P(A|B)P(B) + P(A|B^C)P(B^C)}$$\n",
    "\n",
    "-P. 381 is a good example of the formula in use.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Recap:</h1>\n",
    "\n",
    "-The conditional probability of an event B given the event A is: <strong>P(B|A) = P(A and B)/P(A)</strong><br>\n",
    "-The General Addition Rule: <strong>P(A or B) = P(A) + P(B) - P(A and B)</strong><br>\n",
    "-The General Multiplication Rule: <strong>P(A and B) = P(A) x P(B|A)</strong><br>\n",
    "-Events A and B are independent when: <strong>P(B|A) = P(B)</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter15:Random Variables</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Center: The Expected Value</h1>\n",
    "\n",
    "*<strong>Random Variable:</strong> Numeric value based on a random event.  We use a capital letter, to denote a random variable.\n",
    "\n",
    "*<strong>Discrete Random Variable:</strong> If we can list all the outcomes, we might formally call a random variable a discrete random variable.  Otherwise, it's considered a <strong>continuous</strong> random variable.\n",
    "\n",
    "*<strong>Probability Model</strong>: The collection of all the possible values and the probabilities that they occur is called the probability model for a discrete random variable.\n",
    "\n",
    "-Suppose for example, that the death rate in any year is 1 out of every 1000 people and that another 2 out of 1000 suffer some kind of disability.  We can display the probability model for this insurace policy in a table like:\n",
    "\n",
    "<table border=\"2px solid black\">\n",
    "    <th>Policyholder Outcome</th>\n",
    "    <th>Payout x</th>\n",
    "    <th>Probability P(X = x)</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">Death</td>\n",
    "        <td style=\"text-align:center\">10,000</td>\n",
    "        <td style=\"text-align:center\">$\\frac{1}{1000}$</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td style=\"text-align:center\">Disability</td>\n",
    "        <td style=\"text-align:center\">5000</td>\n",
    "        <td style=\"text-align:center\">$\\frac{2}{1000}$</td>\n",
    "    </tr>    \n",
    "        <tr>\n",
    "        <td style=\"text-align:center\">Neither</td>\n",
    "        <td style=\"text-align:center\">0</td>\n",
    "        <td style=\"text-align:center\">$\\frac{997}{1000}$</td>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n",
    "-We can't predict what will happen in any given year, but we can say what we expect to happen.  To do this, we need the probability model.  The expected value of a policy is a parameter of this model.  In fact, it is the mean.\n",
    "\n",
    "-We will use $\\mu$ (for population mean) or E(X) for expected value.  We assume the expected values are known and simply calculate the expected values from them.  For the example pertaining to the table above:\n",
    "\n",
    "$$\\mu = E(X) = \\frac{10,000(1) + 5000(2) + 0(997)}{1000}$$\n",
    "\n",
    "-This can be rewritten:\n",
    "\n",
    "$$\\mu = E(X)$$\n",
    "$$= \\$10,000(\\frac{1}{1000}) + \\$5000(\\frac{2}{1000}) + \\$0(\\frac{997}{1000})$$\n",
    "$$= \\$20$$\n",
    "\n",
    "-It's easy to calculate the expected value of a (discrete) random variable--just multiply each possible value by the probability that it occurs, and find the sum:\n",
    "$$\\mu = E(X) = \\sum xP(x)$$\n",
    "\n",
    "-Be sure that every possible outcome is included in the sum.  Verify that you have a valid probability model to start with--the probabilities should each be between 0 and 1 and should sum to one. (good example p.391).\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Spread: The Standard Deviation</h1>\n",
    "\n",
    "-It is important to anticipate the variability of a random variable, therefore we should know the standard deviation.\n",
    "\n",
    "-For standard deviation of data, we calculated the standard deviation by computing the deviation from the mean and squaring it.  We can do the same with random variables:\n",
    "\n",
    "<table border=\"2px solid black\">\n",
    "    <th>Policyholder Outcome</th>\n",
    "    <th>Payout x</th>\n",
    "    <th>Probability P(X = x)</th>\n",
    "    <th>Deviation<br>(x - $\\mu$)</th>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">Death</td>\n",
    "        <td style=\"text-align:center\">10,000</td>\n",
    "        <td style=\"text-align:center\">$\\frac{1}{1000}$</td>\n",
    "        <td style=\"text-align:center\">(10,000 - 20) = 9980</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td style=\"text-align:center\">Disability</td>\n",
    "        <td style=\"text-align:center\">5000</td>\n",
    "        <td style=\"text-align:center\">$\\frac{2}{1000}$</td>\n",
    "        <td style=\"text-align:center\">(5000 - 20) = 4980</td>\n",
    "    </tr>    \n",
    "        <tr>\n",
    "        <td style=\"text-align:center\">Neither</td>\n",
    "        <td style=\"text-align:center\">0</td>\n",
    "        <td style=\"text-align:center\">$\\frac{997}{1000}$</td>\n",
    "        <td style=\"text-align:center\">(0 - 20) = -20</td>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n",
    "-Next, we square each deviation.  The variance is the expected value of those squared deviations, so we multiply each by the appropriate probability and sum those products.  This gives us the <strong>variance</strong> of X.  Here is what it looks like:\n",
    "\n",
    "$$Var(X) = 9980^2(\\frac{1}{1000}) + 4980^2(\\frac{2}{1000}) + (-20)^2(\\frac{997}{1000}) = 149,600$$\n",
    "\n",
    "-Finally, we take the square root to get the standard deviation:\n",
    "$$SD(X)=\\sqrt{149,6000}\\approx\\$386.78$$\n",
    "\n",
    "-The formulas rewritten:\n",
    "\n",
    "$$\\sigma^2=Var(X)=\\sum (x - \\mu)^2P(x)$$\n",
    "$$\\sigma=SD(X)=\\sqrt{Var(X)}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px\">Shifting and Combining Random Variables</h1>\n",
    "\n",
    "-We know that adding or subtracting a constant from data shifts the mean but doesn't change the variance or standard deviation.  The same is true of random variables:\n",
    "\n",
    "$$E(X +/- c) = E(X) +/- c$$\n",
    "$$Var(X +/- c) = Var(X)$$\n",
    "\n",
    "-Multiplying or dividing all data values by a constant changes both the mean and the standard deviation by the same factor.  Variance, being the square of the standard deviation, changes by the square of the constant.  The same is true of random variables.  <strong>In general, multiplying each value of a random variable by a constant multiplies the mean by that constant and the variance by the square of the constant:</strong>\n",
    "\n",
    "$$E(aX) = aE(X)$$\n",
    "$$Var(aX)=a^2Var(X)$$\n",
    "\n",
    "-Is the risk of insuring two people the same as the risk of insuring one person for twice as much?  If the random variables are independent, there is a simple Addition Rule for variances: <strong>The variance of the sum of two independent random variables is the sum of their individual variances</strong>\n",
    "\n",
    "-In general:\n",
    "<ul>\n",
    "    <li>The mean of the sum of two random variables is the sum of the means.</li>\n",
    "    <li>The mean of the difference of two random variables is the difference of the means.</li>\n",
    "    <li>If the random variables are independent, the variance of their sum or difference is always the sum of the variances</li>\n",
    "</ul>\n",
    "\n",
    "-Example on p.399"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px\">Correlation and Covariance</h1>\n",
    "\n",
    "-If X is a random variable with expected value E(X) = $\\mu$ and Y is a random variable with expected value E(Y) = v, then the covariance of X and Y is defined as:\n",
    "$$Cov(X,Y)=E((X - \\mu)(Y - v))$$\n",
    "\n",
    "-The covariance, like the correlation, measures how X and Y vary together.  Covariance has a few properties worth remembering:\n",
    "<ul>\n",
    "    <li>Cov(X,Y)=Cov(Y,X)</li>\n",
    "    <li>Cov(X,X)=Var(X)</li>\n",
    "    <li>Cov(cX,dY)=cdCov(X,Y),for any constants c and d</li>\n",
    "    <li>Cov(X,Y)=E(XY)-$\\mu$v</li>\n",
    "    <li>if X and Y are independent, then Cov(X,Y)=0, it is however possible for two variables to have zero covariance yet not be independent.</li>\n",
    "</ul>\n",
    "\n",
    "-The covariance gives us the extra information we need to find the variance of the sum or difference of two random variables when they are not independent:\n",
    "\n",
    "$$Var(X +/- Y) = Var(X) + Var(Y) +/- 2Cov(X,Y)$$\n",
    "\n",
    "-When X and Y are independent, their covariance is zero.\n",
    "\n",
    "-Covariance, unlike correlation doesn't have to be between -1 and 1.  If X and Y have large values, the covariance will be large as well.  We can divide the covariance by each of the standard deviations to get the <strong>correlation</strong>:\n",
    "\n",
    "$$Corr(X,Y) = \\frac{Cov(X,Y)}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "-Correlation of random variables is usually denoted as $p$.  Correlation of random variables has many of the properties we saw in Chapter 6 for the correlation coefficient $r$.\n",
    "\n",
    "<ul>\n",
    "    <li>The sign of a correlation coefficient gives the direction of the association between random variables.</li>\n",
    "    <li>Correlation is always between -1 and +1.</li>\n",
    "    <li>Correlation treats X and Y symmetrically.  The correlation of X with Y is the same as the correlation of Y with X.</li>\n",
    "    <li>Unlike covariance, correlation has no units.</li>\n",
    "    <li>Correlation is not affected by changes in the center or scale of either variable.</li>\n",
    "</ul>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px\">Continuous Random Variables</h1>\n",
    "\n",
    "-A Normal random variable differs from a discrete random variable because a Normal random variable can take on any value.  It's an example of a <strong>continuous random variable</strong>.\n",
    "\n",
    "-For a Normal, we find the probability that it lies between two values by finding the area under the Normal Curve.  For any continous random variable, this curve doesn't give the probability directly as the probability models for discrete random variables did.  The probability for any single z-score, becomes 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
