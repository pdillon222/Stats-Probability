{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:40px;\">Stats: Data & Models (De Veaux, Velleman & Bock)[Chapters 25-28]</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter25:Inferences for Regression</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Population and the Sample</h1>\n",
    "\n",
    "-When we found a confidence interval for the mean, we could imagine a single, true underlying value for the mean.  When we tested whether two means or two proprtions were equal, we imagined a true underlying difference.\n",
    "\n",
    "-How does this translate to draw inference for regression?  We write the idealized line with Greek letters and consider the coefficients (the slope and intercept) to be parameters: $\\beta$ is the intercept and $\\beta_1$ is the slope.  Corresponding to our fitted line $\\hat{y}=b_0+b_1x$, we write:\n",
    "\n",
    "$$\\mu_y=\\beta_0+\\beta_1x$$\n",
    "\n",
    "-We are using $\\mu$ instead of $\\hat{y}$ because this is a model.  The model places the means of the distributions for each x on the same straight line.\n",
    "\n",
    "-Of course, $\\mu_y$ are not at the same means, so there will be lots of errors (positive or negative).  Because these are model errors, we use a Greek letter and denote them by $\\epsilon$.\n",
    "\n",
    "-When we put errors into the equation, we can account for each individual y:\n",
    "\n",
    "$$y=\\beta_0+\\beta_1+\\epsilon$$\n",
    "\n",
    "-The equation is now true for each data point, so the model gives a value of y for any value of x.\n",
    "\n",
    "-We estimate the $\\beta$'s by finding a regression line, $\\hat{y}=\\beta_0+\\beta_1x$ as we did earlier in chapter 7.\n",
    "\n",
    "-The residuals, $e=y-\\hat{y}$ are the sample-based versions of the errors, $\\epsilon$, we will use them to help us assess the regression model.\n",
    "\n",
    "-We know that the least squares regression will give reasonable estimates of the parameters of this model from a random sample of data.  Our challenge is to account for our uncertainty in how well they do.  For that, we need to make some assumptions about the model and the errors.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "-We should be careful about the order in which we check the assumptions.  If our initial assumptions are not true, it makes no sense to check the later ones.  We will number them to keep them in order.\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Linearity Assumption:</strong>  If the true relationship is far from linear and we use a straight line to fit the data, our entire analysis will be useless, so we always check this first.  The 'Straight Enough Condition' is satisfied if a scatterplot looks straight.  It's easier to see violations of the SEC by looking at a scatterplot of the residuals against x or against the predicted values.  This plot will have a horizontal direction, and should have no pattern if the condition is satisfied.  Of course, we also must be dealing with quantitative data for a linearity test to be valid.</li>\n",
    "    <li><strong>Independence Assumption:</strong>  The errors in the true underlying regression model must be independent of each other.  As usual, there's no way to be sure that the Independence Assumption is true.  We want our individuals to represent a larger population (ideally) and to be randomly sampled from that population.</li>\n",
    "    <li><strong>Equal Variance Assumption:</strong>  The variability of y should be about the same for all values of x.  Earlier (chapter 7), we looked at the standard deviation of the residuals ($s_e$) to measure the size of the scatter.  Now we'll need this standard deviation to build confidence intervals and test hypotheses.  The standard deviation of the residuals is the building block for the standard errors of all the regression parameters.  But it makes sense only if the scatter of the residuals is the same everywhere.  In effect, the standard deviation of the residuals \"pools\" information from across all of the individual distributions at each x-value, and pooled estimates are appropriate only when they combine information for groups with the same variance.</li>\n",
    "    <li><strong>Normal Population Assumption:</strong>  We assume the errors around the idealized regression line at each value of x follow a Normal model.  We need this assumption so that we can use a Student's t-model for inference.  As we have done before when using the Student's t, we'll settle for the residuals satisfying the Nearly Normal Condition and the Outlier Condition.  The Normality Assumption becomes less important as the sample size grows, because the model is about means and the Central Limit Theorem takes over.</li>\n",
    "</ol>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Which Come First: The Conditions or the Residuals?</h1>\n",
    "\n",
    "-In regression, there's a small catch.  The best way to check many of the conditions is with the residuals, but we get the residuals only after we compute the regression.  Before we compute the regression, however, we should check at least one of the conditions, working in order:\n",
    "\n",
    "<ol>\n",
    "    <li>Make a scatterplot of the data to check the Straight Enough Condition.  (If the relationship is curved, try re-expressing the data, or stop).</li>\n",
    "    <li>If the data are straight enough, fit a regression and find the predicted values $\\hat{y}$, and the residuals, $e$.</li>\n",
    "    <li>Make a scatterplot of the residuals against x or against the predicted values.  This plot should should have no pattern.  Check in particular for any bend (which would suggest that the data weren't all that straight after all), for any thickening (or thinning), and of course, for any outliers.  (If there are any outliers, and you can correct them or justify removing them, do so and go back to step 1, or consider performing two regressions--one with and one without the outliers.)</li>\n",
    "    <li>If the data are measured over time, plot the residuals against time to check for evidence of patterns that might suggest they are not independent.</li>\n",
    "    <li>If the scatterplots look OK, then make a histogram and Normal probability plot of the residuals to check the Nearly Normal Condition.</li>\n",
    "    <li>If all conditions seem to be reasonably satisfied, go ahead with inference.</li>\n",
    "</ol>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Intuition About Regression Inference</h1>\n",
    "\n",
    "-So far, we have attempted to calculate regression for sample distributions, but have yet to discuss where the standard errors for the slope or intercept come from.  We kno that if we had collected similar data on a different random sample of men, the slope and intercept would be different.  Each sample would have produced its own regression line, with slightly different $b_0$'s and $b_1$'s.  This sample to sample variation is what generates the sampling distributions for the coefficients.  \n",
    "\n",
    "-There is obviously only one regression model; each sample regression is trying to estimate the same parameters; $\\beta_0$ and $\\beta_1$.  We expect any sample to produce a $b_1$ whose expected value is the true slope is the $\\beta_1$.  What about its standard deviation?  What aspects of the data affect how much the slope (and intercept) vary from sample to sample?\n",
    "\n",
    "-The spread around the regression line is measured with the <strong>residual standard deviation</strong>, $s_e$.  You can alwas find $s_e$ in the regression output, often just labeled s.  Sometimes it is called the \"standard error\", although that's not quite right and could be considered a misinterpretation of the $s_e$ notation.\n",
    "\n",
    "-When we first saw this formula in Chapter 7, we said that it looks a lot like the standard deviation of y, only subtracting the predicted values rather than the mean and dividing by (n-2) instead of (n-1):\n",
    "\n",
    "$$s_e=\\sqrt{\\frac{\\sum(y-\\hat{y})^2}{(n-2)}}$$\n",
    "\n",
    "-<strong>The less scatter around the line, the smaller the residual standard deviation and the stronger the relationship between x and y.</strong>\n",
    "\n",
    "-Some people prefer to assess the strength of a regression by looking at $s_e$ rather than $R^2$.  After all, $s_e$ has the same units as y, and because it's the standard deviation of errors around the line, it tells you how close the data are to our model.  By contrast, $R^2$ is the proportion of the variation of y accounted for by x.  It is typically wise to look at both.\n",
    "\n",
    "-Finally, it shouldn't be any surprise that having a larger sample size, n, gives more consistent estimates from sample to sample.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Standard Error for the Slope</h1>\n",
    "\n",
    "-Three aspects of the scatterplot, affect the standard error of the regression slope:\n",
    "\n",
    "<ul>\n",
    "    <li>Spread around the line: $s_e$</li>\n",
    "    <li>Spread of x values: $s_x$</li>\n",
    "    <li>Sample size: $n$</li>\n",
    "</ul>\n",
    "\n",
    "-These are in fact the only things that affect the standard error of the slope.  Although you'll probably never have to calculate it by hand, the formula for the actual <strong>standard error</strong> is:\n",
    "\n",
    "$$SE(b_1)=\\frac{s_e}{\\sqrt{n-1}s_x}$$\n",
    "\n",
    "-The error standard deviation, $s_e$, is in the numerator, since spread around the line increases the slope's standard error.  The denominator has both a sample size term, $\\sqrt{n-1}$, and $s_x$, because increasing either of these decreases the slope's standard error.\n",
    "\n",
    "-We know the $b_1$'s vary from sample to sample.  As you'd expect, their sampling distribution model is centered at $\\beta_1$, the slope of the idealized regression line.  Now we can estimate its standard deviation with $SE(b_1)$.  What about is shape?  The Central Limit Theorem and Gosset's T come to the rescue again.  <strong>When we standardize the slopes by subtracting the model mean and dividing by their standard error, we get a Student's t-model, this time with (n-2) degrees of freedom</strong>:\n",
    "\n",
    "$$\\frac{b_1-\\beta_1}{SE(b_1)}\\sim t_{n-2}$$\n",
    "\n",
    "-A sampling distribution for regression slopes:  When the conditions are met, the standardized estimated regression slope, \n",
    "\n",
    "$$t = \\frac{b_1-\\beta_1}{SE(b_1)}$$\n",
    "\n",
    "-follows a Student's t-model with (n-2) degrees of freedom.  We estimate the standard error with:\n",
    "\n",
    "$$SE(b_1)=\\frac{s_e}{\\sqrt{n-1}s_x}$$,\n",
    "\n",
    "$$s_e=\\sqrt{\\frac{\\sum(y-\\hat{y})^2}{n-2}}$$\n",
    "\n",
    "-n is the number of data values, and $s_x$, is the ordinary standard deviation of the x-values\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">What About the Intercept?</h1>\n",
    "\n",
    "-The same reasoning applies for the intercept.  We could write:\n",
    "\n",
    "$$\\frac{b_0-\\beta_0}{SE(b_0)}\\sim t_{n-2}$$\n",
    "\n",
    "-and use it to construct confidence intervals and test hypotheses, but often the value of the intercept isn't something that we care about--or have a null hypothesis for.  The intercept usually isn't interesting.  Most hypothesis tests and confidence intervals for regression are about the slope.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Regression Inference</h1>\n",
    "\n",
    "-The usual null hypothesis about the slope is that it's equal to 0.  This basically states that there is no linear association between x and y.\n",
    "\n",
    "-To test $H_0:\\beta_1=0$, we find:\n",
    "\n",
    "$$t_{n-2}=\\frac{b_1-0}{SE(b_1)}$$\n",
    "\n",
    "-This is just like every t-test we've seen: a difference between the statistic and its hypothesized value, divided by its standard error.\n",
    "\n",
    "-We can build a confidence interval in the usual way, as an estimate plus or minus a margin of error.  As always, the margin of error is just the product of the standard error and a critical value.  Here the critical values come from the t-distribution with (n-2) degrees of freedom, so a 95% confidence interval for $\\beta$ is:\n",
    "\n",
    "$$b_1\\pm t^*_{n-2}xSE(b_1)$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Standard Errors for Predicted Values</h1>\n",
    "\n",
    "-Now that we have standard errors, we can use those to construct a confidence interval for the predictions and to report our uncertainty honestly.  A confidence interval can tell us how precise our y prediction could actually be.  The precision depends on how we ask the question pertaining to y: <strong>Do we want to know the mean y-value for all cases with an x-value at a certain point?  Or do we want to estimate the y value for a particular x value?</strong>\n",
    "\n",
    "-We start with the same prediction in both cases.  We are predicting the value for a new individual, one that was not part of the original data set.  To emphasize this, we'll call this x-value \"x sub new\" and write it $x_v$.  The regression equation thus becomes:\n",
    "\n",
    "$$\\hat{y}=b_0+b_1x_v$$\n",
    "\n",
    "-Now that we have the predicted value, we construct both intervals around this same number.  Both intervals take the form:\n",
    "\n",
    "$$\\hat{y}_v\\pm t^*_{n-2}xSE$$\n",
    "\n",
    "-The intervals differ because they have different standard errors.  Our choice of ruler depends on which interval we want.\n",
    "\n",
    "-When there's more spread around the line, the standard error will be larger.  If we're less certain of the slope, we'll be less certain of our predictions.  If we have more data (n larger), our predictions will be more precise.  Finally, predictions farther from the mean of x will have more variability than predictions closer to it.  It's alot easier to predict a data point near the middle of the data set than far from the center.  Because the factors are independent of each other, we can add their variances to find the total variability.  The resulting formula of the standard error of the predicted mean value explicitly takes into account each of the factors:\n",
    "\n",
    "$$SE(\\hat{\\mu}_v)=\\sqrt{SE^2(b_1)x(x_v-\\bar{x})^2+\\frac{s^2_e}{n}}$$\n",
    "\n",
    "-Remember that individual values vary more than means, so the standard error for a single predicted value has to be larger than the standard error for the mean.  In fact, the standard error of a single predicted value has an extra source of variability: the variation of individuals around the predicted mean.  That appears as the extra variance term, $s^2_{e}$, at the end under the square root:\n",
    "\n",
    "$$SE(\\hat{y}_v)=\\sqrt{SE^2(b_1)x(x_v-\\bar{x})^2+\\frac{s^2_e}{n}+s^2_{e}}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Confidence Intervals for Predicted Values</h1>\n",
    "\n",
    "-We can predict two kinds of confidence intervals: \n",
    "\n",
    "-Good worked example is on p.706\n",
    "\n",
    "<ol>\n",
    "    <li>A confidence interval for the predicted mean value at $x_v$</li>\n",
    "    <li>A prediction interval for an individual with a particular x-value</li>\n",
    "</ol>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Logistic Regression</h1>\n",
    "\n",
    "-Regression bases on a the logistic curve is called <strong>logistic regression</strong>.  The equation for logistic regression can be written like an ordinary regression on a transformed y-variable:\n",
    "\n",
    "$$ln(\\frac{\\hat{p}}{1-\\hat{p}})=b_0+b_1x$$\n",
    "\n",
    "-The expression on the left-hand side gives the logistic curve.  The logarithm used is the natural, or base $e$, log, although that doesn't really matter to the shape of the model.\n",
    "\n",
    "-Logistic regression models the logarithm of the odds as a linear function of x.  It's the combination of the ratio and the logarithm that gives us a nice S-curved shape.\n",
    "\n",
    "$$ln(\\frac{\\hat{p}}{1-\\hat{p}})=b_0+b_1x$$ for $\\hat{p}$ gives:\n",
    "\n",
    "$$\\hat{p}=\\frac{1}{1+e^{-(b_0+b_1x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter26:Analysis of Variance</h1>\n",
    "\n",
    "-We want to compare differences in groups a bit more formally than before, by testing a hypothesis.  What would the null hypothesis be?  It would seem natural to say that for $H_0$, all the group means are equal.  We know that the actuality of seeing this null-hypothesis is nearly impossible, as there should almost always be sample-to-sample variance among groups.  We want to see, statistically, whether differences as large as those observed in the experiment could naturally occur by chance in groups that have equal means.  If we find that the differences are so large that they would occur only very infrequently in groups that actually have the same means, then as we've done with other hypothesis tests, we will reject the null hypothesis.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Testing Whether the Means of Several Groups are Equal</h1>\n",
    "\n",
    "-In Chapter 22 we saw how to use a t-test to see whether two groups have equal means.  We compared the difference in the means to a standard error estimated from all the data.  And when we were willing to assume that the underlying group variances were equal, we pooled the data from the two groups to find the standard error.  Now that we have more groups, we can't just look at differences in the means.  \n",
    "\n",
    "-How much should means vary?  If we look at how much the data themselves vary, this should be a good starting point as reference.  If the underlying means are actually different, we'd expect that variation to be larger.\n",
    "\n",
    "-We can actually build a hypothesis test, to check whether the variation in the means is bigger than we'd expect it to be just from random fluctuations.  <strong>We wil need a new sampling distribution model, called the F-model.</strong>\n",
    "\n",
    "-Using the F-test, we will compare the differences between the means of the groups with the variation within the groups.  When the differences between means are large compared with the variation within the groups, we reject the null hypothesis and conclude that the means are not equal.\n",
    "\n",
    "-How can we make a comparison of the means to the data variance more precise statistically?  All the tests we've seen so far have compared differences of some kind with a ruler based on an estimate of variation.  We have done this so far by looking at the ratio of the statistic to that variation estimate.  Here, the differences among the means will show up in the numerator, and the ruler we compare them with will be based on the underlying standard deviation--that is, on the variability within the treatment groups.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">How Different Are They?</h1>\n",
    "\n",
    "-The challenge here, is that we can't take a simple difference as we could when we only had two groups.  With only two groups, we naturally took the difference between their means as the numerator of the t-test.  How can we generalize this to more than two groups?  We must measure how much the groups vary.\n",
    "\n",
    "-If the null hypothesis were true, then each of the treatment means among groups would estimate the same underlying mean.  <strong>We can treat estimated means as if they were observations and simply calculate their (sample) variance.  This variance is the measure we'll use to assess how different the group means are from each other.</strong>\n",
    "\n",
    "-The more the group means resemble each other, the smaller this variance will be.  The more they differ, the larger this variance will be.  The null hypothesis:\n",
    "\n",
    "$$H_0:\\mu_1=\\mu_2=\\mu_3=\\mu_4=\\mu$$\n",
    "\n",
    "-We know that the variance of a sample mean is $\\frac{\\sigma^2}{n}$.  Still, how do we determine if the variance of the observations is significant?  We will need a specific kind of hypothesis test.  The details of this test, due to Sir Ronald Fisher in the early 20th century, are truly ingenious, and may be the most amazing statistical result of that century.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Ruler Within</h1>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
