{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:40px;\">Stats: Data & Models (De Veaux, Velleman & Bock)[Chapters 19-21]</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter19:Testing Hypotheses About Proportions</h1>\n",
    "\n",
    "-How can we determine the validity of the proportion derived from a sample?  We answer questions about samples by testing hypotheses about models.                                     \n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Hypotheses</h1>\n",
    "\n",
    "-Typically, it is wise to be skeptical about a hypothesis until the data can show us otherwise.  <strong>Hypotheses</strong> are models that we adopt temporarily--until we can test them once we have data.\n",
    "\n",
    "-The starting hypothesis to test, is called the <strong>null hypothesis</strong>.  Null, because it assumes nothing has changed.  We denote it with $H_0$.  If we were trying to test whether the average rate of a series of samples had changed from 20%, our null hypothesis would be that $H_0$=.2.\n",
    "\n",
    "-The <strong>alternative hypothesis</strong>, which we denote by $H_A$, contains the values of the parameter that we consider plausible if we reject the null hypothesis.\n",
    "\n",
    "-How much different should a rate be in order for us to reject the null hypothesis $H_0$?  When we think about how big the change has to be, we consider the standard deviation.\n",
    "\n",
    "-Why use standard deviation and not standard error?  We reserve the term standard error for the estimate of the standard deviation of the sampling distribution.  However, with hypothesis testing we are not estimating, we have a $p$ value from our null hypothesis model.  To remind us that the parameter value comes from the null hypothesis, it is sometimes written as $p_0$ and the standard deviation as SD($\\hat{p})=\\sqrt{\\frac{p_0 q_0 }{n}}$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">P-Values</h1>\n",
    "\n",
    "-The fundamental step in our reasoning is the question \"Are the data surprising, given the null hypothesis?\".  <strong>The key calculation is to determine exactly how likely the data we observed would be given that the null hypothesis were a true model of the world</strong>\n",
    "\n",
    "-We want to find the probability of seeing data like these (or something even less likely) given that the null hypothesis is true.  This probability tells us how surprised we'd be to see the data we collected if the null hypothesis is true.  We call this the <strong>P-value</strong>.\n",
    "\n",
    "-When the P-value is small enough, it means that it's very unlikely we'd observe data like these if our null hypothesis were true.  The model we started with (the null hypothesis) and the data we collected are at odds with each other, so we have to make a choice.  Either the null hypothesis is correct and we've just seen something remarkable, or the null hypothesis is wrong, and we were wrong to use it as the basis for computing our P-value.  On the other hand, if you believe in data more than in assumptions, then, given that choice, you should reject the null hypothesis.\n",
    "\n",
    "-When the P-value is high, we haven't seen anything unlikely or surprising at all.  The data are consistent with the model from the null hypothesis, and we have no reason to reject the null hypothesis.  Importantly, we haven't proven that the null hypothesis is true.  The most we can say is that it doesn't appear to be false.\n",
    "\n",
    "-As a courtroom example: when a defendant goes before judge and jury, the null hypothesis $H_0$ is that the defendant is innocent (until proven guilty).  If the evidence is too unlikely given this assumption (i.e. the P-value is too small)-- the jury rejects the null hypothesis and finds the defendant guilty.  This is however an important distinction, if there is insufficient evidence to convict the defendant, the jury does not decide that $H_0$ is true and declare the defendant innocent.  They simply say that the defendant is \"not-guilty\", a rejection of $H_A$.\n",
    "\n",
    "-In a hypothesis test, the burden of proof is on the unusual claim.  The null hypothesis is the ordinary state of affairs, so it's the alternative to the null hypothesis that we consider unusual and for which we must marshal evidence.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Reasoning of Hypothesis Testing</h1>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Hypothesis</strong><br>First, we state the null hypothesis.  This is usually the skeptical claim that nothing is different.  To assess how unlikely our data may be, we need a null model.  The null hypothesis specifies a particular parameter value to use in our model.  In shorthand, we write $H_0$: parameter=hypothesized value.  The alternative hypothesis, $H_A$ contains the values of the parameter we consider plausible when we reject the null.</li>\n",
    "    <li><strong>Model</strong><br>To plan a statistical hypothesis test, specify the model you will use to test the null hypothesis and the parameter of interest.  Of course, all models require assumptions, so you will need to state them and check any corresponding conditions.</li>\n",
    "    <li><strong>Mechanics</strong><br>We place the actual calculation of our test statistic from the data.  Different tests we encounter will have different formulas and different test statistics.  The ultimate goal of the calculation is to find out how surprising our data would be if the null hypothesis were true.  We measure this by the P-value: the probability that the observed statistic value occurs if the null model is correct.  If the P-value is small enough, we'll reject the null hypothesis.</li>\n",
    "    <li><strong>Conclusion</strong><br>The conclusion in a hypothesis test, is always a statement about the null hypothesis.  The conclusion must state either that we reject or that we fail to reject the null hypothesis.  The size of the effect is always a concern when we test hypotheses.  A good way to look at the effect size is to examine a confidence interval.</li>\n",
    "</ol>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Alternative Alternatives</h1>\n",
    "\n",
    "-We can often have a choice in alternative hypotheses.  If our null hypothesis is $H_0$=.2, then we could have a <strong>two-sided alternative</strong>, in which $H_A\\neq$.20.  For two-sided alternatives, the P-value is the probability of deviating in either direction from the null hypothesis value.\n",
    "\n",
    "-For the same data, the one-sided P-value is half the two-sided P-value.  So a one sided test will reject the null hypothesis more often.  If you aren't sure which to use, a two-sided test is always more conservative.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">P-Values and Decisions: What to Tell ABout a Hypothesis Test</h1>\n",
    "\n",
    "-How small the P-value should be to reject the null-hypothesis is rather context dependent.  Your conclusion about any null-hypothesis should always be accompanied by the P-value of the test.  Always report the P-value to show the strength of the evidence against the hypothesis and the effect size.  To complete an analysis, follow the test with a confidence interval for the parameter of interest to report the size of the effect.\n",
    "\n",
    "TI-84 interlude: \n",
    "-The TI-84 can make quick work of hypothesis tests, and calculate P-value with:\n",
    "1-PropZTest\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter20:Inferences About Means</h1>\n",
    "\n",
    "-Proportions are summaries of individual responses, which had two possible values (\"yes\" and \"no\").  Quantitative data, though, usually report a quantitative value for each individual.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Central Limit Theorem (Again)</h1>\n",
    "\n",
    "-We always center confidence intervals at our best guess of the unknown parameter.  Then we add and subtract a margin of error (ME).  For proportions, we write $\\hat{p}\\text{(+\\-)}ME$.\n",
    "\n",
    "-We found the margin of error as the product of the standard error, SE($\\hat{p}$), and a critical value $z^*$, from the Normal table.  So we had $\\hat{p}\\text{(+/-)}z^*SE(\\hat{p})$.\n",
    "\n",
    "-We knew we could use z because the Central Limit Theorem told us that the sampling distribution model for proportions is Normal.  Now we want to do the same thing for means, and fortunately the Central Limit Theorem told us that the same Normal model works as the sampling distribution for means.\n",
    "\n",
    "-When a random sample is drawn from any population with mean $\\mu$ and sd $\\sigma$, its sample mean $\\hat{y}$ has a sampling distribution with the same mean $\\mu$, but whose standard deviation is $\\frac{\\sigma}{\\sqrt{n}}$ (and we write $\\sigma(\\hat{y})=\\frac{\\sigma}{\\sqrt{n}})$\n",
    "\n",
    "-No matter what population the random sample comes from, the shape of the sampling distribution is approximately Normal as long as the sample size is large enough.  The larger the sample used, the more closely the Normal approximates the sampling distribution for the mean.\n",
    "\n",
    "-Unlike proportions, with means we are forced the estimate the population standard deviation $\\sigma$.  We therefore estimate the population standard deviation $\\sigma$, with $s$.  The resulting standard error is $SE(\\hat{y})=\\frac{s}{\\sqrt{n}}$.\n",
    "\n",
    "-It became realized, that a new sampling model would be needed for mean samples with smaller n population sizes.  It would be needed to allow for extra variation with larger margins of error and P-values.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Gosset's T</h1>\n",
    "\n",
    "-Gosset, discovered that when $\\frac{s}{\\sqrt{n}}$ was used as an estimate of the standard deviation of the mean, the shape of the sampling model changed.  A new model was derived.\n",
    "\n",
    "-While working for Guinnes, Gosset was allowed to publish his new model under the pseudonym \"Student\".  The important result, is widely known as <strong>Student's t</strong>.\n",
    "\n",
    "-Gosset's sampling model is always bell-shaped, but the details change with different sample sizes.  When the sample size is very large, the model is nearly Normal, but when it's small the tails of the distribution are much heavier than the Normal.  This means that values that are far from the mean are more common and that can be important for small samples.  The Student's t-models form an entire family of related distributions that depend on a parameter known as <strong>degrees of freedom</strong>.  The degrees of freedom of a distribution represent the number of independent quantities that are left after we've estimated the parameters.  Here, it's simply the number of data values, n, minus the number of estimated parameters.  For means, that's just n-1.  We often denote degrees of freedom as df, and the model as $t_{df}$, with degrees of freedom as the subscript.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Degrees of Freedom</h1>\n",
    "\n",
    "-When the formula for standard deviation was introduced, we introduced the n-1 denominator as opposed to n.  It is closely tied to the concept of degrees of freedom.  If only we knew the true population mean, $\\mu$, we could use it in our formula for the sample standard deviation:\n",
    "\n",
    "$$s=\\sqrt{\\frac{\\sum{(y-\\mu)^2}}{n}}$$\n",
    "\n",
    "-We don't know $\\mu$, so we naturally use $\\hat{y}$ in its place.  This however causes a small problem:  For any sample, the data values will generally be closer to their own sample mean, $\\hat{y}$, than to the true population mean $\\mu$.  <strong>From a sample, the individual values will always be closer to the sample mean than to the population mean</strong>.  Therefore, our standard deviation is too small.  The amazing mathematical fact is that we can fix it by dividing by (n-1) instead of by n.  This difference is much more important when n is small.  The t-distribution inherits this number and we call (n-1) the degrees of freedom.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">What Did Gosset See?</h1>\n",
    "\n",
    "-Gosset discovered that for distributions of means with smaller n values, the distributions were thinner in the middle and longer in the tails.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">A Confidence Interval for Means</h1>\n",
    "\n",
    "-To make confidence intervals or test hypotheses for means, we need to use Gosset's model.  For means, it turns out the right value for degrees of freedom is df=(n-1).\n",
    "\n",
    "-When certain assumptions and conditions are met, the standardized sample mean:\n",
    "\n",
    "$$t=\\frac{\\bar{y}-\\mu}{SE(\\bar{y})}$$\n",
    "\n",
    "*Follows a student's t-model with n-1 degrees of freedom.  We estimate the standard deviation with:\n",
    "\n",
    "$$SE(\\bar{y})=\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "-When you use Gosset's model, instead of the Normal model, your confidence intervals will be just a bit wider and your P-values just a bit larger.  That's the correction you need.  By using the t-model, you've compensated for the extra variability in precisely the right way.\n",
    "\n",
    "-When the assumptions and conditions are met, we are ready to find the confidence interval for the population mean, $\\mu$.  The confidence interval is:\n",
    "\n",
    "$$\\bar{y}\\text{(+/-)}t_{n-1}^*xSE(\\bar{y})$$\n",
    "\n",
    "-Where the standard error of the mean is $SE(\\bar{y})=\\frac{s}{\\sqrt{}n}$\n",
    "\n",
    "-The critical value $t_{n-1}^*$ depends on the particular confidence level, C, that you specify and on the number of degrees of freedom, n-1, which we get from the sample size.\n",
    "\n",
    "-Student's t-models are unimodal, symmetric and bell shaped just like the Normal.  But t-models with only a few degrees of freedom have longer tails and a larger standard deviation than the Normal (that's what makes the margin of error bigger.)\n",
    "\n",
    "-As the degrees of freedom increase, the t-models look more and more like the Standard Normal.  In fact the t-model with the infinite degrees of freedom is exactly Normal.  Fortunately, above a few hundred degrees of freedom and it's very hard to tell the difference.  Remember, that if we don't have to estimate $\\sigma$ we can simply use the Normal model.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "-There are certain assumptions we need to use the Student's t-models.\n",
    "\n",
    "<strong>Independence Assumption</strong>: This condition is satisfied if the data arise from a random sample or suitably randomized experiment.  Randomly sampled data, and especially data from a simple random sample are almost surely independent.  If the data don't satisfy the Randomization Condition, then you should think about whether the values are likely to be independent for the variables they are concerned with and whether the sample you have is likely to be representative of the population you wish to learn about.\n",
    "\n",
    "-In the rare event that you have a sample that is more than 10% of the population, you may want to consider using special formulas that adjust for that\n",
    "\n",
    "<strong>Normal Population Assumption</strong>: Student's t-models won't work for small samples that are badly skewed.  For small samples, it's sufficient to check the Nearly Normal Condition: the data should come from a distribution that is unimodal and symmetric.  For a sample size where N < 15, if you find outliers or strong skewness, don't use these methods.\n",
    "\n",
    "-When the sample size is larger than 40 or 50, the t methods are safe to use unless the data are extremely skewed.  Be sure to make a histogram.  If you find outliers in the data, it's always a good idea to perform the analysis twice, once with and once without the outliers.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Using Table T to Find t-Values</h1>\n",
    "\n",
    "-The student's t-model is different for each value of degrees of freedom.  Usually we find critical values and margins of error for Student's t-based intervals and tests with technology.\n",
    "\n",
    "-The only reasonable way to check the Nearly Normal Condition is with graphs of the data.  Make a hisogram of the data and verify that its distribution is unimodal and symmetric and that it has no outliers.\n",
    "\n",
    "-Once we have derived our t-based confidence interval, it is appropriate for us to make a statement like \"90% of intervals that could be found in this way would cover the true value\".\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">A Hypothesis Test for the Mean</h1>\n",
    "\n",
    "-Hypothesis tests for the mean compares the difference between the observed statistic and a hypothesized value to the standard error of the observed statistic.  We've seen that for means, the appropriate probability model to use for P-values is Student's t with (n-1) degrees of freedom.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">One-Sample t-Test for the Mean</h1>\n",
    "\n",
    "-The assumptions and conditions for the one-sample t-test for the mean are the same as for the one-sample t-interval.  We test the hypothesis $H_0:\\mu=\\mu_0$ using the statistic:\n",
    "\n",
    "$$t_{n-1}=\\frac{\\bar{y}-\\mu_0}{SE(\\bar{y})}$$\n",
    "\n",
    "-Where the standard error of $\\bar{y}$ is $SE(\\bar{y})=\\frac{s}{\\sqrt{n}}$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Intervals and Tests</h1>\n",
    "\n",
    "-Confidence intervals and significance tests are built from the same calculations.  They are in fact complementary ways of looking at the same question.  Here's the connection: the confidence interval contains all the null hypothesis values we can't reject with these data.\n",
    "\n",
    "-Confidence intervals and hypothesis tests look at the same problem from two different perspectives.  A hypothesis test starts with a proposed parameter value and asks if the data are consistent with that value.  If the observed statistic is too far from the proposed parameter value, that makes it less plausible that the proposed value is the truth.  So we reject the null hypothesis.  \n",
    "\n",
    "-By contrast, a confidence interval starts with the data and finds an interval of plausible values for where the parameter may lie.\n",
    "\n",
    "-How is the confidence level related to the P-value?  To be precise, a level C confidence interval contains all of the plausible null hypothesis values that would not be rejected if you use a (two-sided) P-value of (1-C) as the cutoff for deciding to reject $H_0$.\n",
    "\n",
    "-Confidence intervals are naturally two-sided, so they correspond to two-sided P-values.  When the hypothesis is one-sided, the interval contains values that would not be rejected using a cutoff P-value of (1-C)/2\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Special Case of Proportions</h1>\n",
    "\n",
    "-The relationship between confidence intervals and hypothesis tests works for almost every inference procedure seen in this book.  WE use the standard error of the statistic as a ruler for both.\n",
    "\n",
    "-Proportions are special however.  When we test a hypothesis about a proportion, we use the hypothesized null value rather than the observed proportion to find the P-value.  Because we don't estimate the value, we call the standard deviation of the proportion SD($\\hat p$)\n",
    "\n",
    "-But when we construct a confidence interval, there's no null hypothesis value.  So, we use the observed $\\hat{p}$ value, and calculate a standard error, SE($\\hat{p}$).  When the hypothesized null value is close to the observed value, the SE and SD are similar, so the usual relationship between the test and the interval works reasonably well.  But if the hypothesized value is quite far from the observed value the relationship between test and interval breaks down.  In that case, the confidence interval is a more appropriate way to describe the population proportion.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Choosing the Sample Size</h1>\n",
    "\n",
    "-How large a sample do we need, the simple answer is \"more\".\n",
    "\n",
    "-For a mean, $ME=t_{n-1}^* x SE(\\bar{y})$ and $SE(\\bar{y})=\\frac{s}{\\sqrt{n}}$\n",
    "\n",
    "$$ME=t_{n-1}^*\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "-But the equation needs s and before we collect the data, we don't know s.  In these cases, a small pilot study can provide some information about the standard deviation.  \n",
    "\n",
    "-Also, without knowing n, you won't know the degrees of freedom so we can't find the critical value $ME=t_{n-1}^*$.  One common approach is to use the corresponding z value from the Normal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter21:More About Tests and Intervals</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Choosing Hypotheses</h1>\n",
    "\n",
    "-An appropriate null hypothesis arises directly from the context of the problem.  It is dictated, not by the data, but by the situation.  One good way to identify both the null and alternative hypotheses is to think about why the study is being done and what we hope to learn from the test.\n",
    "\n",
    "-To write a null hypothesis, identify a parameter and choose a null value that relates to the question at hand.  Although the null usually means no difference or no change, you can't automatically interpret \"null\" to mean zero.  A claim that H0=0 is absurd, instead the null hypothesis is that the true proportion remains unchanged.  You need to find the value for the parameter in the null hypothesis from the context of the problem. \n",
    "\n",
    "-There is a temptation to state your <em>claim</em> as the null hypothesis.  However, you can not prove a null hypothesis so it makes more sense to use what you want to show as the alternative.  This way, if the data supports your claim, the P-value will be small\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">How to Think About P-Values</h1>\n",
    "\n",
    "-A P-value is a conditional probability.  It tells us the probability of getting results at least as unusual as the observed statistic, given that the null hypothesis is true.\n",
    "\n",
    "-We can write: P-value=P(observed statistic value | $H_0$)\n",
    "\n",
    "-We can find the P-value, P(observed statistic value | $H_0$), because $H-0$ gives the parameter values that we need to calculate the required probability.\n",
    "\n",
    "-As tempting as it might be to say that 'a P-value of .03 means there's a 3% chance that the null hypothesis is true, this just isn't right.  All we can say is that, given the null hypothesis, there's a 3% chance of observing the statistic value that we have actually observed (or one more unlike the null value).\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">What to do With a Small P-Value</h1>\n",
    "\n",
    "-We know that a small P-value means that the result we just observed is unlikely to occur if the null hypothesis is true.  So we have evidence against the null hypothesis.\n",
    "\n",
    "-The P-value should serve as a measure of the strength of the evidence against the null hypothesis, but should never serve as a hard and fast rule for decisions.  \n",
    "\n",
    "-A small P-value by itself says nothing about how much lower the rate might be.  The confidence interval provides that information.\n",
    "\n",
    "-Remember that a confidence interval is contructed by:\n",
    "\n",
    "$$SD(\\hat{p})=\\sqrt{\\frac{p_0q_0}{n}}$$\n",
    "\n",
    "$$\\hat{p}\\pm z^*\\sqrt{\\frac{\\hat{p}\\hat{q}}{n}}$$\n",
    "\n",
    "$$z = \\frac{\\hat{p}-p_0}{SD(\\hat{p})}$$\n",
    "\n",
    "-We might like to know the P($H_0$|data), but when you think about it we can't talk about the probability that the null hypothesis is true.  This is because the null hypothesis is not a random event.  So either the null is true, or it is not.\n",
    "\n",
    "-The data however, are random in the sense that if we were to repeat a randomized experiment or draw another random sample, we'd get different data and expect to find a different statistic value.  So we can talk about the probability of the data given the null hypothesis, which is our P-value.\n",
    "\n",
    "-As a point, the lower the P-value, the more comfortable we feel about our decision to reject the null hypothesis, but the null hypothesis doesn't get any more false.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Alpha Levels</h1>\n",
    "\n",
    "-When the P-value is small, it tells us that our data are rare, given the null hypothesis.  We can define \"rare event\" arbitrarily by setting a threshold for our P-value.  If our P-value falls below that point, we'll reject the null hypothesis.  We call such results <strong>statistically significant</strong>.  The threshold is called the </strong>alpha level</strong>.  It's labeled withe the greek letter $\\alpha$.  Common $\\alpha$-levels are .10,.05,.01 and .001.\n",
    "\n",
    "-You have the obligation to consider your alpha level carefully and choose an appropriate one for the situation.\n",
    "\n",
    "-Note that the $\\alpha$ is the threshold, at which point if the P-value is above we choose not to reject the null hypothesis.  You must select the alpha level, before you look at the data.  Otherwise, you could be accused of cheating by tuning your alpha level to suit the data.\n",
    "\n",
    "-the alpha level is also called the significance level.  When we reject the null hypothesis, we say that the test is \"significant at that level\".\n",
    "\n",
    "-When you have not found sufficient evidence to reject the null according to the standard you have established, you should say that \"The data have failed to provide sufficient evidence to reject the null hypothesis\".  Never say, \"We accept the null hypothesis\".\n",
    "\n",
    "-Many statisticians think it better to report the P-value than to base a decision on an arbitrary alpha level.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Practical vs. Statistical Significance</h1>\n",
    "\n",
    "-For large samples, even small, unimportant deviations from the null hypothesis.\n",
    "\n",
    "-On the other hand, if the sample is not large enough, even large financially or scientifically \"significant\" differences may not be statistically significant.\n",
    "\n",
    "-It's good practice to report a confidence interval for the parameter along with the P-value on which we base statistical significance to indicate the range of plausible values for the parameter. \n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Critical Values for Hypothesis Tests</h1>\n",
    "\n",
    "-When we make a confidence interval, we find a <strong>critical value</strong>, $z^*$ of $t^*$ to correspond to our selected confidence level.  Critical values can also be used as a shortcut for hypothesis tests.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">A Confidence Interval for Small Samples</h1>\n",
    "\n",
    "-There are special exact methods that don't require the 10% assumption so they can be used when your sample is a large fraction of the population\n",
    "\n",
    "-TI-84 Interlude:\n",
    "*To find the critical z-value for a 95% confidence interval: invNorm(.025,0,1) -->2.326.\n",
    "\n",
    "-There is also the other extreme, where you have very few successes or failures.  A simple adjustment to the calculation lets us make a 95% confidence interval anyway.\n",
    "\n",
    "-All we do, is add 4 phony observations, two to the successes, two to the failures.\n",
    "\n",
    "-Instead of the proportion $\\hat{p}=\\frac{y}{n}$, we use the adjusted proportion $\\widetilde{p}=\\frac{y+2}{n+4}$\n",
    "\n",
    "-We modify the interval by using these adjusted values for both the center of the interval and the margin of error.  Now the adjusted interval is\n",
    "\n",
    "$$\\widetilde{p}\\pm z^*\\sqrt{\\frac{\\tilde{p}(1-\\tilde{p})}{\\tilde{n}}}$$\n",
    "\n",
    "-This adjusted form gives better performance overall and works much better for proportions near 0 or 1.  It has the additional advantage that we no longer need to check the Success/Failure Condition that $\\hat{np}$ and $\\hat{nq}$ are greater than 10.  This inteval is called the Agresti-Coull interval or the \"plus-four\" interval.\n",
    "\n",
    "-The plus-four method is not the most common way to find a confidence interval for a proportion, but you can use it safely for any sample size--as long as you explain clearly what you have done.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Confidence Intervals and Hypothesis Tests</h1>\n",
    "\n",
    "-Any hypothesized value for the true proportion in a confidence interval is consistent with the data.\n",
    "\n",
    "-Any value outside the confidence interval would make a null hypothesis that we would reject, but we'd feel strongly about values more strongly outside the interval.\n",
    "\n",
    "-Because confidence intervals are naturally two-sided, they correspond to two-sided tests.\n",
    "\n",
    "-the relationship between confidence intervals and one-sided hypothesis tests is a little more complicated.  For a one sided test with $\\alpha$=5%, the corresponding confidence interval has a confidence level of 90%--that's 5% in each tail.  In general, a confidence interval with a confidence level of C% corresponds to a one-sided hyphothesis test with an $\\alpha$ level of $\\frac{1}{2}(100-C)%$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Errors</h1>\n",
    "\n",
    "-Mistakes can be made in two ways: \n",
    "<ol>\n",
    "    <li>The null hypothesis is true, but we mistakenly reject it (Type I error)</li>\n",
    "    <li>The null hypothesis is false, but we fail to reject it (Type II error</li>\n",
    "</ol>\n",
    "\n",
    "-One way to keep the names straight is to remember that we start by assuming the null hypothesis is true, so a Type I error is the first type of mistake that we could make.\n",
    "\n",
    "-In a medical/disease setting, the null hypothesis is usually the assumption that an individual is healthy.  The alternative is that the individual has the disease we are testing for.  Therefore, to reject the null hypothesis that the person is healthy and therefore has a disease is considered a <em>false positive</em>.\n",
    "\n",
    "-A Type II error on the other hand is considered a <em>false negative</em>.\n",
    "\n",
    "-To reject $H_0$, the P-Value must fall below $\\alpha$, when $H_0$ is true, that happens exactly with probability $\\alpha$.  So when you choose level $\\alpha$, you're setting the probability of a Type I error to $\\alpha$.\n",
    "\n",
    "-Consider that a Type I error, can only occur when $H_0$ is true.  When $H_0$ is false but we fail to reject it, we have made a Type II error.  We assign the letter $\\beta$ to the probability of this mistake.\n",
    "\n",
    "-The value of $\\beta$ is more difficult to compute than the value of $\\alpha$ because we don't know the true value of the parameter.  When $H_0$ is true, it specifies a single parameter value.  When it is false, we don't know the parameter value and there are many possible values.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Power</h1>\n",
    "\n",
    "-When the null hypothesis is actually false, we hope our test is strong enough to reject it.  The <strong>power</strong> of a test is the probability that it correctly rejects a false null hypothesis.  When the power is high, we can be confident that we've looked hard enough.  We know that $\\beta$ is the probability that a test fails to reject the null hypothesis, so the power of the test is the probability that it succeeds in rejecting the false null hypothesis: 1-$\\beta$.\n",
    "\n",
    "-Whenever a study fails to reject the null hypothesis, the test's power comes into question.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Effect Size</h1>\n",
    "\n",
    "-The value of the power depends on how far the truth lies from the value we hypothesize.  We call the distance between the null hypothesis value, $p_0$, and the truth $p$, the <strong>effect size</strong>.\n",
    "\n",
    "-A larger effect size is easier to see and results in larger power.  Small effects are more difficult to detect, and will result in more Type II errors and therefore lower power.\n",
    "\n",
    "-The power of a test depends on the size of the effect and the standard deviation.  \n",
    "\n",
    "-For proportions, our test statistic is $\\hat{p}$, and its standard deviation is $\\sqrt{\\frac{pq}{n}}$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">A Picture Worth $\\frac{1}{P(z > 3.09)}$ Words</h1>\n",
    "\n",
    "-It makes intuitive sense that the larger the effect size, the easier it should be to see it.  Obtaining a larger sample size decreases the probability of a Type II error, so it increases the power.  It also makes sense that the more willing to accept a Type I error, the less likely we will be to make a Type II error.\n",
    "\n",
    "-Power=1-$\\beta$\n",
    "\n",
    "-Reducing $\\alpha$ to lower the chance of committing a Type I error will move the critical value, $p^*$ to the right.  This will have the effect of increasing $\\beta$, the probability of a Type II error.\n",
    "\n",
    "-The larger the difference between the hypothesized value, $p_0$ and the true population value, $p$, the smaller the chance of making a Type II error and the greater the power of the test.  If the two proportions are very far apart, the two models will barely overlap.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Reducing Both Type I and Type II Errors</h1>\n",
    "\n",
    "-If we can make both curves narrower, then both the probability of Type I errors and the probability of Type II errors will decrease, and the power of the test will increase.\n",
    "\n",
    "-We can do this, by reducing the standard deviations, which can be accomplished by increasing  the sample size."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
