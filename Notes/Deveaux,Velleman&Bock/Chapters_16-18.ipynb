{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:40px;\">Stats: Data & Models (De Veaux, Velleman & Bock)[Chapters 16-18]</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">TI-84 Interlude:</h1>\n",
    "\n",
    "<strong>Normalpdf(x,$\\mu$,$\\sigma$)</strong>\n",
    "\n",
    "-Returns the probability of a single value of the random variable (x), given known $\\mu$ and $\\sigma$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<strong>Normalcdf(lower_limit,upper_limit,$\\mu$,$\\sigma$)</strong>\n",
    "\n",
    "-Returns the cumulative probability from lower_limit to upper_limit given known $\\mu$ and $\\sigma$.  Technically, it returns the percentage area under the curve of the Normal probability distribution curve.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<strong>Invnorm(a,$\\mu$,$\\sigma$)</strong>\n",
    "\n",
    "-Returns the x-value given the probability region to the left of the a-value.  The inverse normal probability distribution function will find the precise value at a given percent (a) based upon the mean and standard deviation.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<strong>Geometpdf(p,x)</strong>\n",
    "\n",
    "-Used to calculate geometric probability, it solves a specific type of problem that occurs under the following conditions:\n",
    "\n",
    "<ol>\n",
    "    <li>A specific event has two outcomes: success or failure</li>\n",
    "    <li>The event is going to keep happening until success occurs</li>\n",
    "    <li>Success or failure is determined randomly with the same probability of success each time the event occurs.</li>\n",
    "    <li>We are interested in finding the specific probability that it takes a specific amount of trials to get a success</li>\n",
    "</ol>\n",
    "\n",
    "*Example Question: Consider a basketball player that always makes a shot with 1/3 probability.  He will keep throwing the ball until he makes a shot.  What is the probability that it takes him 3 shots?\n",
    "\n",
    "*command: geometpdf(1/3,3)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<strong>Geometcdf(p,x)</strong>\n",
    "\n",
    "-Used to calculate geometric probability, it solves a specific type of problem that occurs under the following conditions:\n",
    "\n",
    "<ol>\n",
    "    <li>A specific event has two outcomes: success or failure</li>\n",
    "    <li>The event is going to keep happening until success occurs</li>\n",
    "    <li>Success or failure is determined randomly with the same probability of success each time the event occurs.</li>\n",
    "    <li>We are interested in finding the specific probability that it takes a <strong>at most</strong> specific amount of trials to get a success (x)</li>\n",
    "</ol>\n",
    "\n",
    "*Example Question: Consider a basketball player that always makes a shot with 1/4 probability.  He will keep throwing the ball until he makes a shot.  What is the probability that it takes him at most 4 shots?\n",
    "\n",
    "*command: geometcdf(1/4,4)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<strong>Binompdf(n,p,x)</strong>\n",
    "\n",
    "-Used to calculate the binomial probability.  It solves a specific type of often -encountered probability problem that occurs under the following conditions:\n",
    "\n",
    "<ol>\n",
    "    <li>A specific event has only two outcomes, which can be considered as success or failure</li>\n",
    "    <li>This event is going to repeat a specific number of times, or \"trials\"</li>\n",
    "    <li>Success or failure is determined randomly with the same probability of success each time the event occurs</li>\n",
    "    <li>We are interested in the probability that there are exactly 'n' successes</li>\n",
    "</ol>\n",
    "\n",
    "*Example Question: Consider a couple that intends to have 4 children, what is the probability that 3 of them are girls?\n",
    "\n",
    "<ol>\n",
    "    <li>The event here is a child being born.  It has two outcomes \"boy\" or \"girl\".  We can call either one a success, but we'll choose to be sexist towards guys and call a girl a success in this problem</li>\n",
    "    <li>The event is going to repeat 4 times, so we have 4 trials</li>\n",
    "    <li>The probability of a girl being born is 50% or 1/2 each time</li>\n",
    "    <li>We're interested in the probability that there are exactly 3 successes (3 girls)</li>\n",
    "</ol>\n",
    "\n",
    "*command: binompdf(4,.5,3)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<strong>Binomcdf(n,p,x)</strong>\n",
    "\n",
    "-This command is used to calculate the binomial cumulative probability function.  It solves a specific type of often-encountered probability problem that occurs under the following conditions:\n",
    "\n",
    "<ol>\n",
    "    <li>A specific event has only two outcomes, which can be considered as success or failure</li>\n",
    "    <li>This event is going to repeat a specific number of times, or \"trials\"</li>\n",
    "    <li>Success or failure is determined randomly with the same probability of success each time the event occurs</li>\n",
    "    <li>We are interested in the probability that there are at most 'n' successes</li>\n",
    "</ol>\n",
    "\n",
    "*Example Question: Consider a couple that intends to have 4 children, what is the probability that at most two of them are girls?\n",
    "\n",
    "<ol>\n",
    "    <li>The event here is a child being born.  It has two outcomes \"boy\" or \"girl\".  We can call either one a success, but we'll choose to be sexist towards guys and call a girl a success in this problem</li>\n",
    "    <li>The event is going to repeat 4 times, so we have 4 trials</li>\n",
    "    <li>The probability of a girl being born is 50% or 1/2 each time</li>\n",
    "    <li>We're interested in the probability that there are at most 2 successes (2 girls)</li>\n",
    "</ol>\n",
    "\n",
    "*command: binomcdf(4,.5,2)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<strong>Poissonpdf($\\lambda$,x)</strong>\n",
    "\n",
    "-This command is used to calculate Poisson Distribution probability.  It solves a specific type of problem:\n",
    "\n",
    "<ol>\n",
    "    <li>A specific event happens at a known average rate (X occurrences per time interval)</li>\n",
    "    <li>Each occurrence is independent of the time since the last occurrence</li>\n",
    "    <li>We're interested in the probability that the event occurs a specific number of times in a given time interval</li>\n",
    "</ol>\n",
    "\n",
    "-The poissonpdf() command takes two arguments:  The mean is the average number of times the event will happen during the time interval we're interested in.  The value is the number of times we're interested in the event happening (the output is the probability that the event happens <em>value</em> times in the interval).\n",
    "\n",
    "*Example Question: Consider a point on a city street where an average of 5 cars pass by each minute.  What is the probability that in a given minute, 8 cars will drive by?\n",
    "\n",
    "<ol>\n",
    "    <li>The event is a car passing by, which happens at an average rate of 5 occurrences per time interval (a minute)</li>\n",
    "    <li>Each occurrence is independent of the time since the last occurence</li>\n",
    "    <li>We're interested in the probability that the event occurs 8 times in the time interval</li>\n",
    "</ol>\n",
    "\n",
    "*command: poissonpdf(5,8)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<strong>Poissoncdf($\\lambda$,x)</strong>\n",
    "\n",
    "-This command is used to calculate Poisson Distribution cumulative probability.  It solves a specific type of problem:\n",
    "\n",
    "<ol>\n",
    "    <li>A specific event happens at a known average rate (X occurrences per time interval)</li>\n",
    "    <li>Each occurrence is independent of the time since the last occurrence</li>\n",
    "    <li>We're interested in the probability that the event occurs at most a specific number of times in a given time interval</li>\n",
    "</ol>\n",
    "\n",
    "-The poissonpdf() command takes two arguments:  The mean is the average number of times the event will happen during the time interval we're interested in.  The value is the number of times we're interested in the event happening (the output is the probability that the event happens <em>at most value</em> times in the interval)  Note that you may need to convert the mean so that the time intervals in both cases match up.  This is done by a simple proportion: if the event happens 10 times per minute, it happens 20 times per two minutes.\n",
    "\n",
    "*Example Question: Consider a point on a city street where an average of 5 cars pass by each minute.  What is the probability that in a given minute, no more than 3 cars will drive by?\n",
    "\n",
    "<ol>\n",
    "    <li>The event is a car passing by, which happens at an average rate of 5 occurrences per time interval (a minute)</li>\n",
    "    <li>Each occurrence is independent of the time since the last occurence</li>\n",
    "    <li>We're interested in the probability that the event occurs at most 3 times in the time interval</li>\n",
    "</ol>\n",
    "\n",
    "*command: poissonpdf(5,3)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter16:Probability Models</h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Bernoulli Trials</h1>\n",
    "\n",
    "*<strong>Bernoulli Trials</strong>:  Common examples: tossing a coin, looking for defective products rolling off the assembly line, shooting free throws in a basketball game.\n",
    "\n",
    "-Assuming we have a potential event in which the probability of the outcome is .2, what would be the probability of this event's outcome not occurring positively until the 5th turn?\n",
    "$$P(A(5)) = (1-.2)^4*(.2)$$\n",
    "\n",
    "-Given a positive outcome probability of .2, how many events might we expect to occur before the first positive outcome of this described event? $1/.2$ = 5.  This is however, not as easy to prove as one might think.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Geometric Model</h1>\n",
    "\n",
    "-We want to be able to model how long it will take to achieve the first success in a series of <strong>Bernoulli Trials</strong>.  The model that tells us this probability is called the <strong>Geometric Probability Model</strong>.  Geometric models are completely specified by on parameter, $p$ (the probability of success), and are denoted by <strong>Geom($p$)</strong>.\n",
    "\n",
    "-Since achieving the first success on trial number x requires first experiencing (x - 1) failures, the probabilities are easily expressed by a formula (Geometric Probability Model for Bernoulli Trials (Geom($p$)):\n",
    "\n",
    "$$p = \\text{probability of success} (\\text{and q =} 1 - p = \\text{probability of failure})$$\n",
    "\n",
    "$$X = \\text{number of trials until the first success occurs}$$\n",
    "\n",
    "$$P(X = x) = q^{x-1} p$$\n",
    "\n",
    "$$\\text{Expected value: E(X) = }\\mu = \\frac{1}{p}$$\n",
    "\n",
    "$$\\text{Standard deviation: }\\sigma = \\sqrt{\\frac{q}{p^2}}$$\n",
    "\n",
    "-A table display can clarify the expected probability of X given x number of trials:\n",
    "\n",
    "<table border=\"1px solid black\">\n",
    "    <th>x</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>4</th>\n",
    "    <tr>\n",
    "        <td>P(X = x)</td>\n",
    "        <td>p</td>\n",
    "        <td>qp</td>\n",
    "        <td>$q^2p$</td>\n",
    "        <td>$q^3p$</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Independence</h1>\n",
    "\n",
    "-One of the most important requirements for Bernoulli trials is that the trials be independent.  This can often be a problem when considering samples chosen without replacement.\n",
    "\n",
    "-Probabilities are typically based on finite populations, so selecting from a finite population causes the probabilities to change, making the trials non-independent.\n",
    "\n",
    "-As a rule: if we look at less than 10% of the population, we can pretend that the trials are independent and still calculate probabilities that are quite accurate.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Binomial Model</h1>\n",
    "\n",
    "-We can use Bernoulli trials to answer other questions.  Suppose you buy 5 boxes of cereal, what would be the probability of getting two prizes (given a probability of .2 per box, as a prize box).\n",
    "\n",
    "-We're interested in the number of successes in the 5 trials, so we'll call it X = number of successes.\n",
    "\n",
    "-We want to find P(X = 2).  This is an example of a <strong>Binomial Probability</strong>.  It takes two parameters to define thie <strong>Binomial Model</strong>: the number of trials (n), and the probability of success (p).  Here, n = 5 trials, and p = .2, the probability of winning the prize in the box of cereal.\n",
    "\n",
    "-Exactly 2 successes in 5 trials means 2 successes and 3 failures.  It seems logical the probability should be $(.2)^2(.8)^3$, but it is not that simple.  This calculation would give you the probability of finding two prizes in the first two boxes, and none in that last three, in this specific order.  Fortunately, the possible ways to rearrange the orders are disjoint.  There fore, we can use the Addition Rule and add up the probabilities for all the possible orderings.  Since the probabilities are all the same, we on ly need to konw how many orders are possible.  For small numbers, we can just make a tree diagram and count the branches.  For larger numbers this isn't practical, so we let the computer or calculator do the work.\n",
    "\n",
    "-Each different order in which we can have k successes in n trials is called a \"combination\".  The total number of ways that can happen is written ${n \\choose k}$ or $_n C _k$, pronounced \"n choose k\".\n",
    "\n",
    "$$_nC_K=\\frac{n!}{K!(n-k)!}\\text{where n! = n * (n - 1) * (n - 2) ... * 1 }$$\n",
    "\n",
    "$$_5C_2=\\frac{5!}{2!(5-2)!}=\\frac{\\text{5*4*3*2*1}}{\\text{2*1*3*2*1}}=\\frac{\\text{5*4}}{\\text{2*1}}=10$$\n",
    "\n",
    "-So back to our earlier example, there are 10 ways to get 2 prizes in 5 boxes, and the probability of each is $(.2)^2(.8)^3$  Therefore we determine that:\n",
    "\n",
    "$$P(\\#successes=2)=10(.2)^2(.8)^3=.2048$$\n",
    "\n",
    "-In general, the probability of exactly k successes in n trials is:\n",
    "$$_nC_kp^kq^{n-k}$$\n",
    "\n",
    "-Using this formula, we could find the expected value by adding up xP(X = x) for all values.  Intuitively, it is fairly clear that we could estimate fairly accurately by: \n",
    "\n",
    "$$\\mu=n*p$$\n",
    "\n",
    "-The standard deviation is far less obvious; one can't just rely on intuition.  Fortunately, the formula for the standard deviation boils down to something simple:\n",
    "\n",
    "$$SD(X)=\\sqrt{npq}$$\n",
    "\n",
    "-So if we were looking through 100 boxes of cereal, we would expect to find 20 prize boxes with:\n",
    "\n",
    "$$\\sqrt{100*.8*.2}\\text{ = 4 boxes}$$\n",
    "\n",
    "-To summarize, a Binomial probability model describes the number of successes in a specified number of trials.  (p. 418 is a good example of this in practice combined with the addition rule).\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Approximating the Binomial with a Normal Model</h1>\n",
    "\n",
    "-When dealing with large numbers of trials, making direct calculations of the probabilities becomes tedious (if not impossible).  This is where the normal model comes into play.  If we had a situation such as:\n",
    "\n",
    "$$_{32000}C_{1850}*.06^{1850}*.94^{30150}$$\n",
    "\n",
    "-Where the Binomial model has:\n",
    "\n",
    "$$\\text{mean np = 1920}$$\n",
    "$$\\text{SD = $\\sqrt{npq}$}\\approx 42.48$$\n",
    "\n",
    "-With this approximation, we can find the probability:\n",
    "\n",
    "$$\\text{P(X < 1850) = P}(z < \\frac{1850-1920}{42.48})\\approx P(z < -1.65)\\approx .05$$\n",
    "\n",
    "-We can not however use the Normal model to make estimates of Binomial probabilities.  \n",
    "\n",
    "-A Normal model, is a close enough approximation only for a large enough number of trials.  <strong>What we mean by \"large enough\" depends on the probability of success.  We would need a larger sample if the probability of success were very low (or very high).  The Normal model seems to work pretty well if we expect to see at least 10 successes and 10 failures.  So we check the Success/Failure Condition:</strong>\n",
    "\n",
    "*<strong>The Success/Failure Condition</strong>:  A Binomial model is approximately Normal if we expect at least 10 successes and 10 failures:\n",
    "\n",
    "$$np \\geq 10 \\text{ and nq} \\geq 10$$\n",
    "\n",
    "-The problem is that a Normal model extends infinitely in both directions, but a Binomial model must have between 0 and n successes.  Therefore, if we use a Normal model to approximate a Binomial, we have to cut off its tails.  This is not that important if the cetner of the Normal model is so far from 0 and n that the lost tails have only a negligible area.  <strong>More than three standard deviations</strong> should do it, because a Normal model has little probability past that.  So the mean needs to be at least 3 standard deviations from 0 and at least 3 standard deviations from n.\n",
    "\n",
    "-Reminder: on the TI-84, the estimated percentage is derived by <strong>normalCDF(low_limit,upper_limit,mean,sd)</strong>.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Continuity Correction</h1>\n",
    "\n",
    "-The Binonial is discrete, giving probabilitites for specific counts.  However, the Normal is a continuous random variable that can take on any value.  The probability that a Normal takes on a specific value is 0.  So for a Binomial with n=50 and p=.2, the probability of exactly 10 successes is P(X = 10) = .1398.  But for the Normal, P(X = 10) = 0.  So herein lies a problem.  The box below describes a solution, called the <strong>continuity correction</strong>.\n",
    "\n",
    "-When we use a continuous model to model a set of discrete events, we may need to make an adjustment called the <strong>continuity correction</strong>.\n",
    "\n",
    "-When we use the Normal distribution to approximate discrete events, we go halfway value onf the left and/or the right.  Therefore, we would approximate P(X = 10) by finding P(9.5 $\\leq$ X $\\leq$ 10.5).  For a Binomial (50, .2), $\\mu$ = 10 and $\\sigma$ = 2.83.\n",
    "\n",
    "$$\\text{So }P(9.5\\leq X\\leq 10.5)\\approx P(\\frac{9.5-10}{2.83}\\leq z \\leq \\frac{10.5-10}{2.83})$$\n",
    "\n",
    "$$= P(-0.177 \\leq z \\leq 0.177)$$\n",
    "\n",
    "$$= 0.1405$$\n",
    "\n",
    "$$\\text{By comparison, the exact binomial probability is 0.1398}$$\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Poisson Model</h1>\n",
    "\n",
    "-When rare events occur together or in clusters, people often want to know if that happened just by chance or whether someting else is going on.  If we assume that the events occur independently, we can use a Binomial model to find the probability that a cluster of events like this occurs.  For rare events, p will be quite samll, and when n is large, it may be difficult to compute the probability that a cluster of cases of size x occurs.\n",
    "\n",
    "-For rare events, when np falls much below 10, the Normal Probabilities won't be accurate.\n",
    "\n",
    "-The <strong>Poisson Model</strong> was derived to approximate the Binomial model when the probability of a success, p, is very small, and the number of trials, n, is very large.  Poisson's parameter is denoted by $\\lambda$.  To use the Poisson model to approximate the Binomial, we'll make their means match, so we set $\\lambda=np$.\n",
    "\n",
    "$$\\lambda=\\text{mean number of successes}$$\n",
    "\n",
    "$$E(X)=\\text{number of successes}$$\n",
    "\n",
    "$$P(X=x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}$$\n",
    "\n",
    "$$\\text{Expected value:    }E(X)=\\lambda$$\n",
    "\n",
    "$$\\text{Standard deviation:}SD(X)=\\sqrt\\lambda$$\n",
    "\n",
    "-One nice feature of the Poisson model is that it scales according to the sample size.  One of the consequences of the Poisson model is that as long as the mean rate of occurrences stays constant, the occurrence of past events doesn't change the probability of future events.  This is counterintuitive, but further proof that probability does not change in the frame of small numbers.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Other Continuous Random Variables: The Uniform and the Exponential</h1>\n",
    "\n",
    "<strong>The Uniform Distribution</strong>\n",
    "\n",
    "-For the continuous uniform, we'd want all intervals of the same length to have the same probability.  So, not surprisingly the probability model for the continuous random variable looks flat, and is defined by the formula:\n",
    "\n",
    "$$f(n) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{b-a}  & \\text{if $\\leq x \\leq b$} \\\\[2ex]\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "-In general, the expected value is:\n",
    "\n",
    "$$E(X)=\\frac{a+b}{2}$$\n",
    "\n",
    "-The variance and standard deviations are slightly less intuitive for this model:\n",
    "\n",
    "$$Var(X)=\\frac{(b-a)^2}{12}$$\n",
    "\n",
    "$$SD(X)=\\sqrt{\\frac{(b-a)^2}{12}}$$\n",
    "\n",
    "<strong>The Exponential Model</strong>\n",
    "\n",
    "-If a discrete random variable can be modeled by a Poisson model with rate $\\lambda$, then the times between events can be modeled by the continuous exponential model with the same parameter $\\lambda$.  The mean of the Poisson random variable is $\\lambda$ and the mean of the exponential is $\\frac{1}{\\lambda}$.\n",
    "\n",
    "-Probabilities of an exponential random variable can be found through the probability model.  Fortunately, the probability that x lies between any two values, s and t (s $\\leq$ t), has a particularly easy form:\n",
    "\n",
    "$$P(X \\leq t)=P(0\\leq X \\leq t) = e^{-\\lambda 0}-e^{-\\lambda t}=1-e^{-\\lambda t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter17:Sampling Distribution Models/h1>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Sampling Distribution of a Proportion</h1>\n",
    "\n",
    "-If we survey every possible sample of the US, we could find the proportion $\\hat p$ that correspond to a particular value.\n",
    "\n",
    "-For a simulation, we first have to pick a particular value to be the \"true\" population proportion.  We denote the true population proportion with the letter $p$.\n",
    "\n",
    "-If we were to draw a survey 3 times of 1022 respondents, and we got the values 580, 612 and 604; then our $\\hat p$ values would be: [.568, .599, and .591] respectively.\n",
    "\n",
    "-Each $\\hat p$ comes from a different simulated sample.  The distribution we get, if we were to see all proportions from all possible samples is called the <strong>sampling distribution</strong> of the proportions.\n",
    "\n",
    "-The histogram is unimodal, symmetric, and centered at $p$.  It's very fortunate, that a Normal model is just the right one for the histogram of sampling proportions (this has also been proved mathematically).\n",
    "\n",
    "-A <strong>sampling distribution model</strong> for how a statistic from a sample varies from sample to sample allows us to quantify that variation and to make statements about where to think the corresponding population parameter is.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Which Normal?</h1>\n",
    "\n",
    "-To use a Normal model, we need to specify two parameters: its mean and standard deviation.  The center of the histogram is naturally at $p$.  So that is what we will use for the mean of the Normal model.  As a reminder, there is no information about $\\sigma$ from knowing the value of $\\mu$ for quantitative data.\n",
    "\n",
    "-There is however a special fact about proportions that makes the standard deviation easy to find.  Once we know the mean, $p$, (and write q = 1 - p) we automatically also know the standard deviation of $\\hat p$.  We saw in Chapter 16 that for a Binomial model, the standard deviation of the number of successes is $\\sqrt{npq}$.\n",
    "\n",
    "-Now, we want the standard deviation of the proportion of successes, $\\hat p$.  The proportion is just the number of successes divided by the number of trials, n, so the standard deviation is also divided by n:\n",
    "\n",
    "$$\\sigma(\\hat p)=SD(\\hat p)=\\frac{\\sqrt{npq}}{n}=\\sqrt{\\frac{pq}{n}}$$\n",
    "\n",
    "-When we draw simple random samples of n individuals, the proportions we find will vary from sample to sample.  As long as n is reasonably large, we can model the distribution of these sample proportions with a probability model that is:\n",
    "\n",
    "$$N(p,\\sqrt{\\frac{pq}{n}})$$\n",
    "\n",
    "-Because we have a Normal model, we can use the 68-95-99.7 Rule or look up the exact probabilities using a table or technology.  For example, we know that 95% of Normally distributed values are within roughly two standard deviations of the mean, so we should not be surprised if various polls that may appear to ask the same question report a variety of results.  Such sample-to-sample variation is sometimes called <strong>sampling error</strong>.  It is not really error at all, more so it is variability and is therefore also referred to as <strong>sampling variability</strong>.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">When Does the Normal Model Work?  Assumptions and Conditions</h1>\n",
    "\n",
    "-The Normal model becomes a better and better representation of the sampling distribution as the size of the samples gets bigger.  Sample proportions of larger samples are modeled well by the Normal model as long as p isn't too close to 0 or 1.\n",
    "\n",
    "-Populations with a true proportion, $p$, close to 0 or 1 can be a problem.  If $p$ is close to 0, the distribution will be highly skewed to the right.  Vice-versa, if it is close to 1 it will be higly skewed to the left.\n",
    "\n",
    "-When does the Normal model with mean $P$ and deviation $\\sqrt{\\frac{pq}{n}}$ work well as a model for the sampling distribution of a sample proportion?  We can summarize those cases by checking the following assumptions and conditions:\n",
    "\n",
    "<ul>\n",
    "    <li><strong>The Independence Assumption:</strong>The individuals in the samples must be independent of each other.  If not the observations will resemble each other too much and distort the standard deviation.  You can't know if an assumption is true or not, but you can check to see whether the data were collected in a way that makes this assumption plausible.</li>\n",
    "    <li><strong>Randomization Condition:</strong>If your data come from an experiment in which subjects were randomly assigned to treatments or from a survey based on a simple random sample, then the data satisfy the randomization condition.  If not, you'll need to think carefully about whether it is honest to consider your cases independent with respect to the variables of interest and whether the cases you have are representative of the population you care about</li>\n",
    "    <li><strong>10% Condition:</strong>It can be a problem to sample too much of the population.  Once you've sampled more than 10% of the population, the remaining individuals are no longer really independent of each other.  We've seen that n has to be large enough and that p (and q=1-p as well) can't be too close to 0 or 1.  We put those two concerns together and check for at least 10 \"successes\" and 10 \"failures\".  That is, make sure that $\\hat{np}$ and $\\hat{nq}$ are both at least 10</li>\n",
    "    <li><strong>Success/Failure Condition:</strong>  You should have at least 10 successes and 10 failures in your data.  The Success/Failure condition encourages a large sample size, but the 10% condition warns against sampling too much of a limited population.  Nevertheless this apparent conflict is rarely a problem.</li>\n",
    "</ul>\n",
    "\n",
    "-If these conditions are met, then you can use the Normal model to model the distribution of the sample proportion.\n",
    "\n",
    "-(Worked example on p.449-450).\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Sampling Distribution of Other Statistics</h1>\n",
    "\n",
    "-As the sample size gets larger, each sample average is more likely to be closer to the population mean.  Also, the shape of the distribution becomes closer to the Normal model.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Central Limit Theorem: The Fundamental Theorem of Statistics</h1>\n",
    "\n",
    "-The sampling distribution of any mean becomes more nearly Normal as the sample size grows.  All we need is for the observations to be independent and collected with randomization.  We don't even care about the shape of the population distribution.  This results in a fundamental theorem of statistics called the <strong>Central Limit Theorem</strong>.  To reiterate: The distribution of means of repeated random samples gets closer and closer to a Normal model as the sample size grows, regardless of the shape of the population distribution.\n",
    "\n",
    "-What about a really bimodal population, one that consists of only 0's and 1's?  The Central Limit Theorem says that even means of samples from this population will follow a Normal sampling distribution model.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "-The Central Limit Theorem requires essentially the same assumptions as we saw for modeling proportions:\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Independence Assumption:</strong>The sampled values must be independent of each other.</li>\n",
    "    <li><strong>Randomization Condition:</strong>Sample should be randomly selected, and no more than 10% of the population.</li>\n",
    "    <li><strong>Sample Size Condition:</strong>There is no one size fits all condition.  If the population is unimodal and symmetric, even a fairly small sample size is ok.  If the population is strongly skewed, it can take a fairly large sample for the Normal model to describe the distribution of sample means.  So we can see that context is important, when considering sample size.</li>\n",
    "</ul>\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">But Which Normal?</h1>\n",
    "\n",
    "-Any Normal, is specified by its mean and standard deviation.  For proportions, the sampling distribution is centered at the population proportiono.  For means, it's centered at the population mean.\n",
    "\n",
    "-Means have smaller standard deviations than individuals.  How much smaller?: The standard deviation of y falls as the sample size grows.  Unfortunately, it doesn't drop as fast as we might think.  It only goes down by the square root of the sample size.  That is, the sampling distribution of the mean has a standard deviation equal to:\n",
    "\n",
    "$$SD(\\bar y)=\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "-where $\\sigma$ is the standard deviation of the population.  To Emphasize that this is a standard deviaton parameter of the sampling distribution model for the sample mean, $\\bar{y}$, we write $SD(\\bar{y})$ or $\\sigma(\\bar{y})$.\n",
    "\n",
    "-When a random sample is drawn from any population with mean $\\mu$ and standard deviation $\\sigma$, its sample mean, y, has a sampling distribution with the same $\\mu$, but whose standard deviation is $\\frac{sigma}{\\sqrt{n}}$\n",
    "\n",
    "-We now have two closely related distribution models that we can use when the appropriate assumptions and conditions are met.  Which one we use depends upon which kind of data we have:\n",
    "\n",
    "<ul>\n",
    "    <li><strong>When we have categorical data, we calculate a sample proportion</strong>, $\\hat{p}$; the sampling distribution is roughtly Normal with a mean at the true proportion p and a standard deviation of \n",
    "    $$SD(\\hat{p})=\\sqrt{\\frac{pq}{n}}=\\frac{\\sqrt{pq}}{\\sqrt{n}}$$\n",
    "    </li>\n",
    "    <li><strong>When we have quantitative data, we calculate a sample mean</strong>, $\\bar{y}$, the sampling distribution is roughly Normal with a mean at the true mean, $\\mu$ and a standard deviation of \n",
    "$$SD(\\bar{y})=\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "-The means of these models are easy to remember, so all you need to be careful about are the standard deviations.  Remember that these are standard deviations of the statistics $\\hat{p}$ and $\\bar{y}$.  They both have a square root of n in the denominator.  That tells us that the larger the sample, the less either statistic will vary. \n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Sampling Distributions: A Summary</h1>\n",
    "\n",
    "-Averages are much less variable than individual values.  The fact that both $SD(\\bar{y})=\\frac{\\sigma}{\\sqrt{n}}$ and $SD(\\hat{p})=\\sqrt{(pq/n)}$ confirms this hunch because n is the denominator in both cases.  This shows that the variability of sample means decreases as the sample size increases.  The standard deviation of the sampling distribution declines only with the square root of the sample size and not, for example with 1/n.\n",
    "\n",
    "-the mean of a random sample of 4 has half $(\\frac{1}{\\sqrt{4}}=\\frac{1}{2})$ the standard deviation of an individual data value.  To cut the standard deviation in half again, we'd need a sample of 16, and a sample of 64 to halve it once more.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">The Real World and the Model World</h1>\n",
    "\n",
    "-Now we are left with two distributions to deal with: The first is the real world distribution of the sample, which we might display with a histogram (for quantitative data) or with a bar chart or table (for categorical data).  The second is the math world sampling distribution model of the statistic, a Normal model based on the <strong>Central Limit Theorem</strong>.  Let us not confuse the two.\n",
    "\n",
    "-Remember, the Central Limit Theorem doesn't talk about the distribution of the data from the sample.  <strong>It talks about the distribution of sample means and sample proportions of many different random samples drawn from the same population</strong>.  Of course, the CLT does require that the sample be big enough when the population shape is not unimodal and symmetric.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Sampling Distribution Models--A Summary</h1>\n",
    "\n",
    "-At the heart of all of this, is the idea that the statistic itself is a random variable, we can't know what our statistic will be because it comes from a random sample.  It's just one instance of something that happened for our particular random sample.  A different random sample would have given a different result.  This sample-to-sample variability is what generates the sampling distribution.  The sampling distribution shows us the distribution of possible values that the statistic could have had.  We could simulate that distribution by pretending to take lots of samples.  Fortunately, for the mean and the proportion, the CLT tells us that we can model their sampling distribution directly with a Normal model.\n",
    "\n",
    "-The two basic truths about sampling distributions are:\n",
    "\n",
    "<ol>\n",
    "    <li>Sampling distributions arise becuase samples vary.  Each random sample will contain different cases and, so, a different value of the statistic.</li>\n",
    "    <li>Although we can always simulate a sampling distribution, the Central Limit Theorem saves us the trouble for means and proportions.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:30px;\">Chapter18:Confidence Intervals for Proportions</h1>\n",
    "\n",
    "-Sample proportions are denoted by $\\hat{p}$, compared to the population proportion $p$.\n",
    "\n",
    "-When considering how a sampling proportions outcome might translate to the population proportion, we begin to concern ourselves with the sampling distribution of the sample proportion of a target population.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">A Confidence Interval</h1>\n",
    "\n",
    "-The model for the sampling distribution is approximately Normal (under certain checked assumptions).  It's mean is the population mean $\\mu$.  Typically, we don't know what this is, we do know however that the sampling distribution model of $\\hat{p}$ is centered at $p$, and since it is a proportion we know that the standard deviation is $\\sqrt{\\frac{pq}{n}}$.\n",
    "\n",
    "-Herein lies a problem, since we don't know p, we can't find the true standard deviation of the sampling distribution model.  We do know the observed proportion $\\hat{p}$.  <strong>So for want of something better, we just use what we know, and we estimate</strong>.\n",
    "\n",
    "-<em>When we estimate the standard deviation of a sampling distribution, we call it a <strong>standard error</strong>, and is denoted by</em>.\n",
    "\n",
    "$$SE(\\hat{p})=\\sqrt{\\frac{\\hat{p}\\hat{q}}{n}}$$\n",
    "\n",
    "-<strong>Based on the outcome of this, we can say that about 68% of all samples of n cases will have $\\hat{p}$'s within 1 SE of $p$</strong>.  Unfortunately, we still don't know what our sample proportion is or what our $p$ value is.\n",
    "\n",
    "-The best we can do is surmise an interval, but it is still only a accurate as a probability level.\n",
    "\n",
    "-With SE=3.7, we could make a statement (with some validity) such as: \"We are 95% confident that $p$ is between 24% and 38.2% if $p$ is 30.8%.\"  This type of confidence interval can be considered a <strong>one-proportion z-interval</strong>.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Interpreting Confidence Intervals: What Does 95% Confidence Really Mean?</h1>\n",
    "\n",
    "-As a rule, we expect 95% of all samples having a confidence interval containing the true proportion.\n",
    "\n",
    "-What do we mean when we say we have 95% confidence that our interval contains the true proportion?  We mean that 95% of samples of this size will produce confidence intervals that capture the true proportion.  We could also say that we are 95% confident that the true proportion lies in our interval.\n",
    "\n",
    "-Our uncertainty is about whether the particular sample we have at hand is one of the successful ones or one of the 5% that fail to produce an interval that captures the true value.\n",
    "\n",
    "-The Central Limit Theorem assures us that in the long run, 95% of the intervals contain that true population proportion.\n",
    "\n",
    "-As a reminder, our confidence is about the interval and not about the true proportion\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Margin of Error: Certainty vs. Precision</h1>\n",
    "\n",
    "-The extent of the interval on either side of $\\hat{p}$ is called the <strong>margin of error (ME)</strong>.  Almost any population parameter [mean, proportion, regression-slope] can be estimated with some margin of error.\n",
    "\n",
    "-We will be able to construct a confidence interval that looks like:\n",
    "\n",
    "$$Estimate (+/-)ME$$\n",
    "\n",
    "-For a 95% confidence interval, the ME is 2 SE.  If we wanted to be more confident, capturing perhaps a 99.7% confidence interval we would have to change the ME to 3 SE.\n",
    "\n",
    "-The most commonly chosen confidence intervals are 90%, 95% and 99%.  In theory, any percentage can be used.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Critical Values</h1>\n",
    "\n",
    "-To change the confidence level, we would need to change the number of SE's so that the size of the margin of error corresponds to the new level.  This number of SE's is called the <strong>critical value</strong>.  It's based on the Normal model, so we denote it $z*$.\n",
    "\n",
    "-For a 95% confidence interval, you'll find that the precise critical value is $z*$=1.96.  That is 95% of a Normal model is found within (+/-) 1.96 standard deviations of the mean\n",
    "\n",
    "-TI-84 Interlude:\n",
    "*For finding the z-score or a 95% confidence interval: invNorm(.05/2,0,1) --> yielding $z*$ of 1.96.  \n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Assumptions and Conditions</h1>\n",
    "\n",
    "-We must apply many of the same conditional tests, when making assumptions about confidence intervals in reference to the Normal model.\n",
    "\n",
    "-<strong>Independence Assumption</strong>: If the data were sampled at random or generated from a properly randomized experiment, they satisfy the <strong>Randomization Condition</strong>.  Whether or not you decide that the Independence Assumption is plausible depends on your knowledge of the situation.  It's not exactly one that you can check by looking at the data.\n",
    "\n",
    "-<strong>10% Condition</strong>: If you sample more than 10% of a population, the formula for the standard error won't be quite right.\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Sample Size Assumption</h1>\n",
    "\n",
    "-The model we use for inference for proportions is based on the Central Limit Theorem.  We need to know whether the sample is large enough to make the sampling model for the sample proportions approximately Normal.  It turns out that we need more data as the proportion gets closer and closer to either extreme (0 or 1).\n",
    "\n",
    "-Again with the <strong>Success/Failure Condition</strong>: we must expect at least 10 \"failures\" and 10 \"successes\"\n",
    "\n",
    "<h1 style=\"font-family:Courier;font-weight:bold;font-size:20px;\">Choosing Your Sample Size</h1>\n",
    "\n",
    "-The question of how large a sample to take is an important step in planning any study.  We are now ready to take that step.\n",
    "\n",
    "-Suppose a candidate is planning a poll and wants to estimate voter support within 3% with 95% confidence.  How large a sample would you need.  Considering the margin of error (ME):\n",
    "\n",
    "$$ME=z*\\sqrt{\\frac{\\hat{p}\\hat{q}}{n}}$$\n",
    "\n",
    "$$.03=1.96\\sqrt{\\frac{\\hat{p}\\hat{q}}{n}}$$\n",
    "\n",
    "-We want to find the proper sample size.  To find n, we need a value for $\\hat{p}$.  We don't know $\\hat{p}$, because we haven't taken a sample yet, but we can probably guess the value.  To be safe, we can take the value that makes $\\hat{p}\\hat{q}$ (and therefore n) the largest.  \n",
    "\n",
    "-This value is $\\hat{p}=\\hat{q}=.50$.  Our equation becomes:\n",
    "\n",
    "$$.03=1.96\\sqrt{\\frac{(.5)(.5)}{n}}$$\n",
    "\n",
    "-Thus becoming:\n",
    "\n",
    "$$.03\\sqrt{n}=1.96\\sqrt{(.5)(.5)}$$\n",
    "\n",
    "$$\\sqrt{n}=\\frac{1.96\\sqrt{(.5)(.5)}}{.03}\\approx32.67$$\n",
    "\n",
    "$$n\\approx(32.67)^2\\approx1067.1$$\n",
    "\n",
    "-When computing sample sizes, we always round up (to ensure the confidence level we wanted) and conclude that we need at least 1068 respondents to keep the margin of error as small as 3% with a confidence level of 95%.\n",
    "\n",
    "-Unfortunately, bigger samples cost more money and more effort.  Because the standard error declines only with the square root of the sample size, to cut the standard error in half, we must quadruple the sample size.\n",
    "\n",
    "-Generally, a margin of error of 5% or less is acceptable.  Remember that the sample size is the actual number of respondents, not the number of people a survey went out to."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
